[
  {
    "objectID": "intro.html#example-least-squares",
    "href": "intro.html#example-least-squares",
    "title": "Introduction",
    "section": "Example: Least-Squares",
    "text": "Example: Least-Squares\n\nModel: \\[\ny_i = x_i'\\beta + \\epsilon_i\n\\]\n$Q_n() = _{i=1} (y_i - x_i’)^2"
  },
  {
    "objectID": "intro.html#example-maximum-likelihood",
    "href": "intro.html#example-maximum-likelihood",
    "title": "Introduction",
    "section": "Example: Maximum Likelihood",
    "text": "Example: Maximum Likelihood\n\nModel: \\(y_i\\) has conditional pdf \\(f(y|x_i;\\theta)\\), independent across \\(i\\)\n\\(Q_n(\\theta) = -\\frac{1}{n} \\sum_{i=1}^n \\log\\left(f(y_i | x_i; \\theta)\\right)\\)"
  },
  {
    "objectID": "intro.html#example-generalized-method-of-moments",
    "href": "intro.html#example-generalized-method-of-moments",
    "title": "Introduction",
    "section": "Example: Generalized Method of Moments",
    "text": "Example: Generalized Method of Moments\n\nModel: \\[\n\\Er\\left[g(y_i,x_i,\\theta_0) \\right] = 0\n\\] for moment function \\(g(y_i,x_i,\\cdot): \\Theta \\to \\R^k\\)\n$Q_n() = ({i=1}^n g(y_i,x_i,))’ W ({i=1}^n g(y_i,x_i,))\n\n\nMoment function or conditional density come from some economic model. We will briefly look at some more detailed examples to illustrate."
  },
  {
    "objectID": "intro.html#example-consumption-and-assets-1",
    "href": "intro.html#example-consumption-and-assets-1",
    "title": "Introduction",
    "section": "Example: Consumption and Assets",
    "text": "Example: Consumption and Assets\n\nHansen and Singleton (1982)\nModel \\[\n\\begin{align*}\n\\max_{c_t, q_t} & \\Er\\left[ \\sum_{t=0}^\\infty \\beta^t u(c_t) | \\mathcal{I}_0 \\right] \\\\\n\\text{s.t. } & \\;\\; p_t q_t + c_t \\leq (p_t + d_t)q_{t-1} + y_t\n\\end{align*}\n\\]"
  },
  {
    "objectID": "intro.html#example-consumption-and-assets-2",
    "href": "intro.html#example-consumption-and-assets-2",
    "title": "Introduction",
    "section": "Example: Consumption and Assets",
    "text": "Example: Consumption and Assets\n\nCleverly rearrange first order conditions: \\[\n\\Er\\left[\\beta \\frac{u'(c_{t+1})}{u'(c_t)} \\underbrace{\\frac{p_{t+1} + d_{t+1}}{p_t}}_{R_t} | \\mathcal{I}_s \\right] = 1 \\text{ for } s \\leq t\n\\]"
  },
  {
    "objectID": "intro.html#example-consumption-and-assets-3",
    "href": "intro.html#example-consumption-and-assets-3",
    "title": "Introduction",
    "section": "Example: Consumption and Assets",
    "text": "Example: Consumption and Assets\n\nAssume \\(u(c) = \\frac{c^{1-\\gamma}}{1-\\gamma}\\) \\[\n\\Er\\left[\\beta \\frac{c_{t+1}^{-\\gamma}}{c_t^{-\\gamma}} R_t | \\mathcal{I}_s \\right] = 1 \\text{ for } s \\leq t\n\\]\nModel implies \\[\n\\Er\\left[\\left(\\beta \\frac{c_{t+1}^{-\\gamma}}{c_t^{-\\gamma}} R_t -1 \\right)Z_t \\right] = 0\n\\] for any \\(Z_t \\in \\mathcal{I}_t\\)\nI.e. \\[\ng(\\overbrace{X_t}^{(c_t,c_{t+1},R_t,Z_t)}, \\underbrace{\\theta}_{(\\beta,\\gamma)}) = \\left(\\beta \\frac{c_{t+1}^{-\\gamma}}{c_t^{-\\gamma}} R_t -1 \\right)Z_t\n\\]"
  },
  {
    "objectID": "intro.html#example-random-coefficients-demand-1",
    "href": "intro.html#example-random-coefficients-demand-1",
    "title": "Introduction",
    "section": "Example: Random Coefficients Demand",
    "text": "Example: Random Coefficients Demand\n\nBerry, Levinsohn, and Pakes (1995)\nConsumers choose product: \\[\nj = \\argmax_{j \\in \\{0, ..., J\\}} x_{jt}' (\\bar{\\beta} + \\Sigma \\nu_i) + \\xi_{jt} + \\epsilon_{ijt}\n\\]\n\n\\(\\nu_i \\sim N(0,I_k)\\), \\(\\epsilon_{ijt} \\sim\\) Type I Extreme Value\nUnobserved demand shock \\(\\xi_{jt}\\)"
  },
  {
    "objectID": "intro.html#example-random-coefficients-demand-2",
    "href": "intro.html#example-random-coefficients-demand-2",
    "title": "Introduction",
    "section": "Example: Random Coefficients Demand",
    "text": "Example: Random Coefficients Demand\n\nAggregate demand: \\[\ns_{jt} = \\int \\frac{e^{x_{jt}'(\\bar{\\beta} + \\Sigma \\nu) + \\xi_{jt}}} {\\sum_{k = 0}^J e^{x_{kt}'(\\bar{\\beta} + \\Sigma \\nu) + \\xi_{kt}} } dF\\nu\n\\]"
  },
  {
    "objectID": "intro.html#example-random-coefficients-demand-3",
    "href": "intro.html#example-random-coefficients-demand-3",
    "title": "Introduction",
    "section": "Example: Random Coefficients Demand",
    "text": "Example: Random Coefficients Demand\n\nInstruments \\(Z_{jt}\\) with \\(E[\\xi_{jt} Z_{jt}] = 0\\)\n\\(g(s_{jt},x_{jt}, Z_{jt}, \\bar{\\beta},\\Sigma) = \\left(\\delta_{jt}(s_{\\cdot t}, x_{\\cdot t},\\beta,\\Sigma) - x_{jt}'\\bar{\\beta}\\right) Z_{jt}\\)\nwhere \\(\\delta_{jt}\\) solves \\[\ns_{jt} = \\int \\frac{e^{\\delta_{jt} + x_{jt}'\\Sigma \\nu}} {\\sum_{k = 0}^J e^{\\delta_{kt} + x_{kt}'\\Sigma \\nu}} dF\\nu\n\\]"
  },
  {
    "objectID": "intro.html#example-insurance-and-drug-demand-1",
    "href": "intro.html#example-insurance-and-drug-demand-1",
    "title": "Introduction",
    "section": "Example: Insurance and Drug Demand",
    "text": "Example: Insurance and Drug Demand\n\nEinav, Finkelstein, and Schrimpf (2015)\nRisk-neutral forward-looking individual faces uncertain health shocks, choose whether or not to fill prescriptions\nPrescriptions are defined by \\((\\theta ,\\omega )\\)\n\n\\(\\theta &gt;0\\) is the prescription’s (total) cost\n\\(\\omega &gt;0\\) is the monetized cost of not taking the drug\nArrive at weekly rate \\(\\lambda\\), drawn from \\(G(\\theta ,\\omega)=G_{2}(\\omega |\\theta )G_{1}(\\theta )\\)\n\\(\\lambda\\) follows a Markov process \\(H(\\lambda |\\lambda ^{\\prime })\\)\n\nInsurance defines \\(c(\\theta ,x)\\) – the out-of-pocket cost associated with a prescription that costs $$ when total spending so far is \\(x\\)"
  },
  {
    "objectID": "intro.html#example-insurance-and-drug-demand-2",
    "href": "intro.html#example-insurance-and-drug-demand-2",
    "title": "Introduction",
    "section": "Example: Insurance and Drug Demand",
    "text": "Example: Insurance and Drug Demand\n\nFlow utility \\[\n  u(\\theta ,\\omega ;x)=\\left \\{\n    \\begin{array}{ll}\n      -c(\\theta ,x) & if\\text{ }filled \\\\\n      -\\omega & if\\text{ }not\\text{ }filled%\n    \\end{array}%\n  \\right.\n  \\]\nBellman equation: \\[\n  \\begin{eqnarray*}\n    v(x,t,\\lambda _{t+1}) &=&E_{\\lambda |\\lambda _{t+1}} \\\\\n    &&\\hspace{-1.35in}\\left[\n      \\begin{array}{c}\n        (1-\\lambda )\\delta v(x,t-1,\\lambda )+ \\\\\n        \\lambda \\int \\max\\left \\{\n          \\begin{array}{l}\n            -c(\\theta ,x)+\\delta v(x+\\theta ,t-1,\\lambda ), \\\\\n            -\\omega +\\delta v(x,t-1,\\lambda )%\n          \\end{array}%\n        \\right \\} dG(\\theta ,\\omega )%\n      \\end{array}%\n    \\right]\n  \\end{eqnarray*}\n  \\] with terminal condition \\(v(x,0)=0\\) for all \\(x\\)"
  },
  {
    "objectID": "intro.html#example-insurance-and-drug-demand-3",
    "href": "intro.html#example-insurance-and-drug-demand-3",
    "title": "Introduction",
    "section": "Example: Insurance and Drug Demand",
    "text": "Example: Insurance and Drug Demand\n\nEstimate by simulated method of moments\n\nSimulate model\nMinimize difference between observed summary statistics and summary statistics in simulated data"
  },
  {
    "objectID": "intro.html#implementing-1",
    "href": "intro.html#implementing-1",
    "title": "Introduction",
    "section": "Implementing",
    "text": "Implementing\n\nNeed to go from mathematical description of model to code\nAll examples need to minimize an objective function\n\nHelpful to compute both objective function and its derivatives\n\nRandom coefficients demand model also needs to solve nonlinear equations and compute integrals\nDrug example needs numeric dynamic programming - function approximation and numeric integration"
  },
  {
    "objectID": "equationsolving.html#nonlinear-equations-1",
    "href": "equationsolving.html#nonlinear-equations-1",
    "title": "Solving Nonlinear Equations",
    "section": "Nonlinear Equations",
    "text": "Nonlinear Equations\n\n\\(F: \\R^n \\to \\R^n\\)\nWant to solve for \\(x\\) \\[\nF(x) = 0\n\\]"
  },
  {
    "objectID": "equationsolving.html#example-blp",
    "href": "equationsolving.html#example-blp",
    "title": "Solving Nonlinear Equations",
    "section": "Example: BLP",
    "text": "Example: BLP\n\nShare equation \\[\ns_{j} = \\int \\frac{e^{\\delta_{j} + x_{j}'\\Sigma \\nu}} {\\sum_{k = 0}^J e^{\\delta_{k} + x_{k}'\\Sigma \\nu}} dF\\nu\n\\]\n\\(J\\) equations to solve for \\(\\delta = (\\delta_{1t}, ..., \\delta_{Jt})\\)"
  },
  {
    "objectID": "equationsolving.html#newtons-method",
    "href": "equationsolving.html#newtons-method",
    "title": "Solving Nonlinear Equations",
    "section": "Newton’s Method",
    "text": "Newton’s Method\n\n\\(F(x)\\) differentiable with Jacobian \\(F'(x)\\)\nAlgorithm:\n\nInitial guess \\(x_0\\)\nUpdate based on first order expansion \\[\n\\begin{align*}\nF(x_{s+1}) \\approx F(x_s) + F'(x_s)(x_{s+1} - x_s) = & 0 \\\\\nx_{s+1} = & x_s + F'(x_s)^{-1} F(x_s)\n\\end{align*}\n\\]\nRepeat until \\(\\Vert F(x_s) \\Vert \\approx 0\\)"
  },
  {
    "objectID": "equationsolving.html#simple-idea-many-variations",
    "href": "equationsolving.html#simple-idea-many-variations",
    "title": "Solving Nonlinear Equations",
    "section": "Simple Idea, Many Variations",
    "text": "Simple Idea, Many Variations\n\nStep size\n\nInitial guess \\(x_0\\)\nUpdate based on first order expansion \\[\n\\begin{align*}\nF(x_{s+1}) \\approx F(x_s) + F'(x_s)(x_{s+1} - x_s) = & 0 \\\\\nx_{s+1} = x_s + {\\color{red}{\\lambda}} F'(x_s)^{-1} F(x_s)\n\\end{align*}\n\\]\nRepeat until \\(\\Vert F(x_s) \\Vert \\approx 0\\)\n\nline search or trust region"
  },
  {
    "objectID": "equationsolving.html#simple-idea-many-variations-1",
    "href": "equationsolving.html#simple-idea-many-variations-1",
    "title": "Solving Nonlinear Equations",
    "section": "Simple Idea, Many Variations",
    "text": "Simple Idea, Many Variations\n\n\nInitial guess \\(x_0\\)\nUpdate based on first order expansion \\[\n\\begin{align*}\nF(x_{s+1}) \\approx F(x_s) + F'(x_s)(x_{s+1} - x_s) = & 0\n\\end{align*}\n\\] approximately solve \\[\nF'(x_s) A = F(x_s)\n\\] update \\[\nx_{s+1} = x_s + \\lambda {\\color{red}{A}}\n\\]\nRepeat until \\(\\Vert F(x_s) \\Vert \\approx 0\\)\n\nEspecially if \\(F'(x_s)\\) is large and/or sparse"
  },
  {
    "objectID": "equationsolving.html#simple-idea-many-variations-2",
    "href": "equationsolving.html#simple-idea-many-variations-2",
    "title": "Solving Nonlinear Equations",
    "section": "Simple Idea, Many Variations",
    "text": "Simple Idea, Many Variations\n\n\nInitial guess \\(x_0\\)\nUpdate based on first order expansion \\[\n\\begin{align*}\nF(x_{s+1}) \\approx F(x_s) + F'(x_s)(x_{s+1} - x_s) = & 0 \\\\\nx_{s+1} = x_s + F'(x_s)^{-1} F(x_s)\n\\end{align*}\n\\]\nRepeat until \\({\\color{red}{\\Vert F(x_{s+1}) \\Vert &lt; rtol \\Vert\nF(x_0) \\Vert + atol }}\\)"
  },
  {
    "objectID": "equationsolving.html#simple-idea-many-variations-3",
    "href": "equationsolving.html#simple-idea-many-variations-3",
    "title": "Solving Nonlinear Equations",
    "section": "Simple Idea, Many Variations",
    "text": "Simple Idea, Many Variations\n\n\nInitial guess \\(x_0\\)\nUpdate based on first order expansion\n\ncompute \\(F'(x_s)\\) using:\n\nhand written code or\nfinite differences or\nsecant method \\(F'(x_s) \\approx \\frac{F(x_s) - F(x_{s-1})}{\\Vert x_s - x_{s-1} \\Vert}\\) or\nautomatic differentiation \\[\n\\begin{align*}\nF(x_{s+1}) \\approx F(x_s) + F'(x_s)(x_{s+1} - x_s) = & 0 \\\\\nx_{s+1} = x_s + F'(x_s)^{-1} F(x_s)\n\\end{align*}\n\\]\n\n\nRepeat until \\(\\Vert F(x_{s+1}) \\Vert \\approx 0\\)"
  },
  {
    "objectID": "equationsolving.html#simple-idea-many-variations-4",
    "href": "equationsolving.html#simple-idea-many-variations-4",
    "title": "Solving Nonlinear Equations",
    "section": "Simple Idea, Many Variations",
    "text": "Simple Idea, Many Variations\n\nKelley (2022) is thorough reference for nonlinear equation solving methods and their properties\nNonlinearSolve.jl gives unified interface for many methods"
  },
  {
    "objectID": "equationsolving.html#blp-share-equation",
    "href": "equationsolving.html#blp-share-equation",
    "title": "Solving Nonlinear Equations",
    "section": "BLP share equation",
    "text": "BLP share equation\n\n@doc raw\"\"\"\n    share(δ, Σ, dFν, x)\n\nComputes shares in random coefficient logit with mean tastes `δ`, observed characteristics `x`, unobserved taste distribution `dFν`, and taste covariances `Σ`.\n\n# Arguments\n\n- `δ` vector of length `J`\n- `Σ` `K` by `K` matrix\n- `dFν` distribution of length `K` vector\n- `x` `J` by `K` array\n\n# Returns\n\n- vector of length `J` consisting of $s_1$, ..., $s_J$\n\"\"\"\nfunction share(δ, Σ, dFν, x, ∫ = ∫cuba)\n  J,K = size(x)\n  (length(δ) == J) || error(\"length(δ)=$(length(δ)) != size(x,1)=$J\")\n  (K,K) === size(Σ) || error(\"size(x,2)=$K != size(Σ)=$(size(Σ))\")\n  function shareν(ν)\n    s = δ + x*Σ*ν\n    s .-= maximum(s)\n    s .= exp.(s)\n    s ./= sum(s)\n    return(s)\n  end\n  return(∫(shareν, dFν))\nend\n\nusing HCubature\nfunction ∫cuba(f, dx; rtol=1e-4)\n  D = length(dx)\n  x(t) = t./(1 .- t.^2)\n  Dx(t) = prod((1 .+ t.^2)./(1 .- t.^2).^2)\n  hcubature(t-&gt;f(x(t))*pdf(dx,x(t))*Dx(t), -ones(D),ones(D), rtol=rtol)[1]\nend\n\nusing SparseGrids, FastGaussQuadrature, Distributions\nfunction ∫sgq(f, dx::MvNormal; order=5)\n  X, W = sparsegrid(length(dx), order, gausshermite, sym=true)\n  L = cholesky(dx.Σ).L\n  sum(f(√2*L*x + dx.μ)*w for (x,w) ∈ zip(X,W))/(π^(length(dx)/2))\nend\n\n∫sgq (generic function with 1 method)"
  },
  {
    "objectID": "equationsolving.html#solving-for-delta",
    "href": "equationsolving.html#solving-for-delta",
    "title": "Solving Nonlinear Equations",
    "section": "Solving for \\(\\delta\\)",
    "text": "Solving for \\(\\delta\\)\n\n# create a problem to solve\nusing LinearAlgebra\nJ = 3\nK = 2\nx = randn(J,K)\nC = randn(K,K)\nΣ = C'*C + I\nδ = randn(J)\ndFν = MvNormal(zeros(K), I)\ns = share(δ, Σ, dFν, x, ∫sgq)\n\n# try to recover δ\nusing NonlinearSolve, LinearAlgebra\np = (Σ=Σ, x=x)\nF(d, p) = share([0, d...],p.Σ,dFν, p.x,∫sgq) - s\nd0 = zeros(length(δ)-1)\nprob = NonlinearProblem(F, d0, p)\nsol = solve(prob, show_trace=Val(true), trace_level=TraceAll())\n\nδ = δ[2:end].-δ[1]\nprintln(\"True δ: $(δ)\")\nprintln(\"Solved δ: $(sol.u)\")\nprintln(\"||F(sol.u)||: $(norm(sol.resid))\")\nprintln(\"Error: $(norm(sol.u - δ))\")\n\n\nAlgorithm: NewtonRaphson(\n   descent = NewtonDescent()\n)\n\n----     -------------        -----------          -------             \nIter     f(u) inf-norm        Step 2-norm          cond(J)             \n----     -------------        -----------          -------             \n0        3.54967143e-01       5.92878775e-322      Inf                 \n1        4.13880262e-02       7.72383583e-01       2.73588247e+00      \n2        7.64865333e-03       4.31612562e-01       8.06142988e+00      \n3        1.14967474e-03       1.14838074e-01       1.42242944e+01      \n4        1.65278023e-04       4.87198810e-02       2.44051669e+01      \n5        5.85421573e-06       9.24852841e-03       3.32829238e+01      \n6        8.40818404e-09       3.51367336e-04       3.57326286e+01      \n7        1.75137682e-14       5.05626714e-07       3.58325861e+01      \nFinal    1.75137682e-14      \n----------------------      \nTrue δ: [-0.7263649359693549, 0.4686811504051215]\nSolved δ: [-0.7263649359683523, 0.46868115040542213]\n||F(sol.u)||: 2.2115203043548142e-14\nError: 1.0467479481157482e-12"
  },
  {
    "objectID": "equationsolving.html#alternative-algorithms",
    "href": "equationsolving.html#alternative-algorithms",
    "title": "Solving Nonlinear Equations",
    "section": "Alternative algorithms",
    "text": "Alternative algorithms\n\nsolr = solve(prob, RobustMultiNewton(), show_trace=Val(true), trace_level=TraceAll())\n\n\nAlgorithm: TrustRegion(\n   trustregion = GenericTrustRegionScheme(method = RadiusUpdateSchemes.Simple),\n   descent = Dogleg(newton_descent = NewtonDescent(), steepest_descent = SteepestDescent())\n)\n\n----     -------------        -----------          -------             \nIter     f(u) inf-norm        Step 2-norm          cond(J)             \n----     -------------        -----------          -------             \n0        3.54967143e-01       6.54784263e-310      Inf                 \n1        3.36075646e-01       3.98826683e-02       2.73588247e+00      \n2        2.97527757e-01       7.97653366e-02       2.84184080e+00      \n3        2.17869604e-01       1.59530673e-01       3.08804897e+00      \n4        5.25891857e-02       3.19061346e-01       3.75535293e+00      \n5        6.77935485e-03       2.22938570e-01       6.56376700e+00      \n6        1.72409426e-03       1.54193501e-01       1.24526403e+01      \n7        2.80691101e-04       6.27258034e-02       2.18669326e+01      \n8        1.55370564e-05       1.50461351e-02       3.17764284e+01      \n9        5.87411523e-08       9.28602780e-04       3.55687426e+01      \n10       8.49598170e-13       3.53247281e-06       3.58317226e+01      \n11       1.11022302e-16       5.10928003e-11       3.58327304e+01      \nFinal    1.11022302e-16      \n----------------------      \n\n\nretcode: Success\nu: 2-element Vector{Float64}:\n -0.7263649359693556\n  0.468681150405121\n\n\n\nusing FixedPointAcceleration\nG(δ,p) = log.(s) - log.(share([0,δ...], Σ, dFν, x, ∫sgq))\nprobfp = NonlinearProblem(G, d0, nothing)\nsolfp = solve(probfp, FixedPointAccelerationJL(algorithm=:Anderson, m=1) )\n\nretcode: Failure\nu: 2-element Vector{Float64}:\n 0.0\n 0.0"
  },
  {
    "objectID": "integration.html#example-random-coefficients-demand-1",
    "href": "integration.html#example-random-coefficients-demand-1",
    "title": "Integration",
    "section": "Example: Random Coefficients Demand",
    "text": "Example: Random Coefficients Demand\n\nBerry, Levinsohn, and Pakes (1995)\nConsumers choose product: \\[\nj = \\argmax_{j \\in \\{0, ..., J\\}} x_{jt}' (\\bar{\\beta} + \\Sigma \\nu_i) + \\xi_{jt} + \\epsilon_{ijt}\n\\]\n\n\\(\\nu_i \\sim N(0,I_k)\\), \\(\\epsilon_{ijt} \\sim\\) Type I Extreme Value\nUnobserved demand shock \\(\\xi_{jt}\\)"
  },
  {
    "objectID": "integration.html#example-random-coefficients-demand-2",
    "href": "integration.html#example-random-coefficients-demand-2",
    "title": "Integration",
    "section": "Example: Random Coefficients Demand",
    "text": "Example: Random Coefficients Demand\n\nAggregate demand: \\[\ns_{jt} = \\int \\frac{e^{x_{jt}'(\\bar{\\beta} + \\Sigma \\nu) + \\xi_{jt}}} {\\sum_{k = 0}^J e^{x_{kt}'(\\bar{\\beta} + \\Sigma \\nu) + \\xi_{kt}} } dF\\nu\n\\]"
  },
  {
    "objectID": "integration.html#example-random-coefficients-demand-3",
    "href": "integration.html#example-random-coefficients-demand-3",
    "title": "Integration",
    "section": "Example: Random Coefficients Demand",
    "text": "Example: Random Coefficients Demand\n\nInstruments \\(Z_{jt}\\) with \\(E[\\xi_{jt} Z_{jt}] = 0\\)\n\\(g(s_{jt},x_{jt}, Z_{jt}, \\bar{\\beta},\\Sigma) = \\left(\\delta_{jt}(s_{\\cdot t}, x_{\\cdot t},\\beta,\\Sigma) - x_{jt}'\\bar{\\beta}\\right) Z_{jt}\\)\nwhere \\(\\delta_{jt}\\) solves \\[\ns_{jt} = \\int \\frac{e^{\\delta_{jt} + x_{jt}'\\Sigma \\nu}} {\\sum_{k = 0}^J e^{\\delta_{kt} + x_{kt}'\\Sigma \\nu}} dF\\nu\n\\]"
  },
  {
    "objectID": "integration.html#shares-version-1",
    "href": "integration.html#shares-version-1",
    "title": "Integration",
    "section": "Shares: version 1",
    "text": "Shares: version 1\n\nfunction share(δ, Σ, dFν, x)\n  function shareν(ν)\n    s = exp.(δ .+ x*Σ*ν)\n    return(s./sum(s))\n  end\n  return(∫(shareν, dFν))\nend\n\nshare (generic function with 1 method)\n\n\n\n\n∫(shareν, dFν) is not a function that exists, will create it next\nclear?\ncorrect?\nrobust?"
  },
  {
    "objectID": "integration.html#shares-version-2",
    "href": "integration.html#shares-version-2",
    "title": "Integration",
    "section": "Shares: version 2",
    "text": "Shares: version 2\n\n@doc raw\"\"\"\n    share(δ, Σ, dFν, x)\n\nComputes shares in random coefficient logit with mean tastes `δ`, observed characteristics `x`, unobserved taste distribution `dFν`, and taste covariances `Σ`.\n\n# Arguments\n\n- `δ` vector of length `J`\n- `Σ` `K` by `K` matrix\n- `dFν` distribution of length `K` vector\n- `x` `J` by `K` array\n\n# Returns\n\n- vector of length `J` consisting of $s_1$, ..., $s_J$\n\"\"\"\nfunction share(δ, Σ, dFν, x)\n  J,K = size(x)\n  (length(δ) == J) || error(\"length(δ)=$(length(δ)) != size(x,1)=$J\")\n  K,K == size(Σ) || error(\"size(x,1)=$J != size(Σ)=$(size(Σ))\")\n  function shareν(ν)\n    s = δ .+ x*Σ*ν\n    s .-= maximum(s)\n    s .= exp.(s)\n    s ./= sum(s)\n    return(s)\n  end\n  return(∫(shareν, dFν))\nend\n\nMain.Notebook.share\n\n\n\n\nadds documentation\nsome error checking on inputs\nprotects against overflow in exp\nAre these changes needed & desirable?"
  },
  {
    "objectID": "integration.html#integration-monte-carlo",
    "href": "integration.html#integration-monte-carlo",
    "title": "Integration",
    "section": "Integration: Monte-Carlo",
    "text": "Integration: Monte-Carlo\n\nusing Distributions, Statistics\n∫mc(f, dx; ndraw=100)=mean(f(rand(dx)) for i in 1:ndraw)\n\n∫mc (generic function with 1 method)\n\n\n\nmodule EC\n\nmutable struct EvaluationCounter\n  f::Function\n  n::Int\nend\n\nEvaluationCounter(f) = EvaluationCounter(f, 0)\n\nfunction (ec::EvaluationCounter)(x)\n  ec.n += 1\n  return ec.f(x)\nend\n\nimport Base: show\nfunction show(io::IO, ec::EvaluationCounter)\n  print(io, \"evaluated $(ec.f) $(ec.n) times\")\nend\n\nfunction reset!(ec::EvaluationCounter)\n  ec.n = 0\nend\n\nend\n\nusing Distributions\ndx = Normal(0, 1)\n\nf = EC.EvaluationCounter(x-&gt;x^2)\ntrueint = 1.0\n\nEC.reset!(f)\nintmc = ∫mc(f, dx, ndraw=100)\n@show intmc, trueint, intmc-trueint\n@show f\nEC.reset!(f)\n\n(intmc, trueint, intmc - trueint) = (0.9369385559524867, 1.0, -0.06306144404751335)\nf = evaluated #6 100 times\n\n\n0"
  },
  {
    "objectID": "integration.html#integration-quasi-monte-carlo",
    "href": "integration.html#integration-quasi-monte-carlo",
    "title": "Integration",
    "section": "Integration: Quasi-Monte-Carlo",
    "text": "Integration: Quasi-Monte-Carlo\n\nUse “low discrepency sequences” to reduce variance of Monte-Carlo integrals\nError is \\(O\\left(\\frac{\\log(n)^d}{n})\\) for integrating \\(d\\) dimensional function with \\(n\\) draws\nSee Owen (2023) for details\n\n\nusing Sobol\nimport Base.Iterators: take\nfunction ∫s(f,dx::AbstractMvNormal;ndraw=100)\n  marginals = [Normal(dx.μ[i], sqrt(dx.Σ[i,i])) for i in 1:length(dx)]\n  invcdf(x) = quantile.(marginals,x)\n  ss = skip(SobolSeq(length(dx)),ndraw)\n  mean(f(invcdf(x)) for x in take(ss,ndraw))\nend\n\nfunction ∫s(f,dx::Normal;ndraw=100)\n  invcdf(x) = quantile(dx,x)\n  ss = skip(SobolSeq(length(dx)),ndraw)\n  mean(f(invcdf(x[1])) for x in take(ss,ndraw))\nend\n\n∫s (generic function with 2 methods)\n\n\n\nf = x-&gt;x^2\nS = 1_000\n@show mean(abs(∫s(f,dx,ndraw=100) - trueint) for s in 1:S)\n@show mean(abs(∫mc(f,dx,ndraw=100) - trueint) for s in 1:S)\n\nmean((abs(∫s(f, dx, ndraw = 100) - trueint) for s = 1:S)) = 0.02875751885407857\nmean((abs(∫mc(f, dx, ndraw = 100) - trueint) for s = 1:S)) = 0.11025243264624399\n\n\n0.11025243264624399"
  },
  {
    "objectID": "integration.html#integration-quadrature",
    "href": "integration.html#integration-quadrature",
    "title": "Integration",
    "section": "Integration: Quadrature",
    "text": "Integration: Quadrature\n\nusing FastGaussQuadrature, LinearAlgebra\nimport Base.Iterators: product, repeated\nfunction ∫q(f, dx::MvNormal; ndraw=100)\n  n = Int(ceil(ndraw^(1/length(dx))))\n  x, w = gausshermite(n)\n  L = cholesky(dx.Σ).L\n  sum(f(√2*L*vcat(xs...) + dx.μ)*prod(ws)\n      for (xs,ws) ∈ zip(product(repeated(x, length(dx))...),\n                        product(repeated(w, length(dx))...))\n        )/(π^(length(dx)/2))\nend\n\n∫q (generic function with 1 method)"
  },
  {
    "objectID": "integration.html#integration-sparse-grid-quadrature",
    "href": "integration.html#integration-sparse-grid-quadrature",
    "title": "Integration",
    "section": "Integration: Sparse Grid Quadrature",
    "text": "Integration: Sparse Grid Quadrature\n\nusing SparseGrids\nfunction ∫sgq(f, dx::MvNormal; order=5)\n  X, W = sparsegrid(length(dx), order, gausshermite, sym=true)\n  L = cholesky(dx.Σ).L\n  sum(f(√2*L*x + dx.μ)*w for (x,w) ∈ zip(X,W))/(π^(length(dx)/2))\nend\n\n∫sgq (generic function with 1 method)"
  },
  {
    "objectID": "integration.html#integration-adaptive-cubature",
    "href": "integration.html#integration-adaptive-cubature",
    "title": "Integration",
    "section": "Integration: Adaptive Cubature",
    "text": "Integration: Adaptive Cubature\n\nusing HCubature\nfunction ∫cuba(f, dx; rtol=1e-4)\n  D = length(dx)\n  x(t) = t./(1 .- t.^2)\n  Dx(t) = prod((1 .+ t.^2)./(1 .- t.^2).^2)\n  hcubature(t-&gt;f(x(t))*pdf(dx,x(t))*Dx(t), -ones(D),ones(D), rtol=rtol)[1]\nend\n\n∫cuba (generic function with 1 method)\n\n\n““” share(δ, Σ, dFν, x)\nComputes\ns_{j} = \\int \\frac{e^{\\delta_{j} + x_{j}'\\Sigma \\nu}} {\\sum_{k = 0}^J e^{\\delta_{k} + x_{k}'\\Sigma \\nu}} dF\\nu"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECON 622: Computational Economics",
    "section": "",
    "text": "Slides\n\nIntroduction\nIntegration\nNonlinear Equation Solving\nBest Practices"
  },
  {
    "objectID": "bestpractices.html#goals",
    "href": "bestpractices.html#goals",
    "title": "Best Practices",
    "section": "Goals",
    "text": "Goals\n\nCorrect\nMaintainable & Extensible\n\nClear\nContained\nConsistent\n\nEfficient"
  },
  {
    "objectID": "bestpractices.html#tools",
    "href": "bestpractices.html#tools",
    "title": "Best Practices",
    "section": "Tools",
    "text": "Tools\n\n\n\n\n\n\n\nGoal\nTool(s)\n\n\n\n\nCorrect\ntests, static analysis\n\n\nMaintainable\nversion control, CI, documentation, dependency management\n\n\nEfficient\nbenchmarks, profiler"
  },
  {
    "objectID": "bestpractices.html#version-control-1",
    "href": "bestpractices.html#version-control-1",
    "title": "Best Practices",
    "section": "Version Control",
    "text": "Version Control\n\nessential\ngit is by far the most popular and what I recommend\ngit hosting services\n\ngithub.com\ngitlab.com\nbitbucket.org\n\nBenefits:\n\nbackup\nedit history\ncoordination"
  },
  {
    "objectID": "bestpractices.html#git",
    "href": "bestpractices.html#git",
    "title": "Best Practices",
    "section": "git",
    "text": "git\n\nUse it\ngit intro and tutorial and webinar"
  },
  {
    "objectID": "bestpractices.html#tool-setup-for-julia",
    "href": "bestpractices.html#tool-setup-for-julia",
    "title": "Best Practices",
    "section": "Tool Setup for Julia",
    "text": "Tool Setup for Julia\n\nOrganize code into a package\nGood advice aimed at scientific computing projects: Julia: Project Workflow\nSetup your package with a project skeleton\n\nPkgTemplates.jl\nBestieTemplate.jl\nDoctorWatson.jl\nHaider, Riesch, and Jirauschek (2021) for similar idea for other languages"
  },
  {
    "objectID": "bestpractices.html#pkgtemplates.jl",
    "href": "bestpractices.html#pkgtemplates.jl",
    "title": "Best Practices",
    "section": "PkgTemplates.jl",
    "text": "PkgTemplates.jl\n\nusing PkgTemplates\nTemplate(interactive=true)(\"SomeNewPackage\")\n\nor\n\nusing PkgTemplates\ntpl = Template(; dir=pwd(),\n               user=\"schrimpf\", # github username\n               authors=[\"Paul Schrimpf\"],\n               plugins=[ProjectFile(), SrcDir(),\n                        Tests(project=true, aqua=true, jet=true),\n                        License(; name=\"MIT\"),\n                        Git(),\n                        GitHubActions(),\n                        Codecov(),\n                        Citation(),\n                        Documenter{GitHubActions}(),\n                        PkgBenchmark(),\n                        Formatter()]\n               )\ntpl(\"SomeNewPackage\")\n\n[ Info: Running prehooks\n[ Info: Running hooks\n  Activating new project at `~/compecon/ECON622/qmd/SomeNewPackage/test`\n   Resolving package versions...\n    Updating `~/compecon/ECON622/qmd/SomeNewPackage/test/Project.toml`\n  [8dfed614] + Test\n    Updating `~/compecon/ECON622/qmd/SomeNewPackage/test/Manifest.toml`\n  [2a0f44e3] + Base64\n  [b77e0a4c] + InteractiveUtils\n  [56ddb016] + Logging\n  [d6f4376e] + Markdown\n  [9a3f8284] + Random\n  [ea8e919c] + SHA v0.7.0\n  [9e88b42a] + Serialization\n  [8dfed614] + Test\n  Activating project at `~/compecon/ECON622/qmd`\n  Activating project at `~/compecon/ECON622/qmd/SomeNewPackage/test`\n   Resolving package versions...\n    Updating `~/compecon/ECON622/qmd/SomeNewPackage/test/Project.toml`\n  [4c88cf16] + Aqua v0.8.7\n    Updating `~/compecon/ECON622/qmd/SomeNewPackage/test/Manifest.toml`\n  [4c88cf16] + Aqua v0.8.7\n  [34da2185] + Compat v4.16.0\n  [0dad84c5] + ArgTools v1.1.1\n  [56f22d72] + Artifacts\n  [ade2ca70] + Dates\n  [f43a241f] + Downloads v1.6.0\n  [7b1f6079] + FileWatching\n  [b27032c2] + LibCURL v0.6.4\n  [76f85450] + LibGit2\n  [8f399da3] + Libdl\n  [ca575930] + NetworkOptions v1.2.0\n  [44cfe95a] + Pkg v1.10.0\n  [de0858da] + Printf\n  [3fa0cd96] + REPL\n  [6462fe0b] + Sockets\n  [fa267f1f] + TOML v1.0.3\n  [a4e569a6] + Tar v1.10.0\n  [cf7118a7] + UUIDs\n  [4ec0a83e] + Unicode\n  [deac9b47] + LibCURL_jll v8.4.0+0\n  [e37daf67] + LibGit2_jll v1.6.4+0\n  [29816b5a] + LibSSH2_jll v1.11.0+1\n  [c8ffd9c3] + MbedTLS_jll v2.28.2+1\n  [14a3606d] + MozillaCACerts_jll v2023.1.10\n  [83775a58] + Zlib_jll v1.2.13+1\n  [8e850ede] + nghttp2_jll v1.52.0+1\n  [3f19e933] + p7zip_jll v17.4.0+2\n  Activating project at `~/compecon/ECON622/qmd`\n  Activating project at `~/compecon/ECON622/qmd/SomeNewPackage/test`\n   Resolving package versions...\n    Updating `~/compecon/ECON622/qmd/SomeNewPackage/test/Project.toml`\n  [c3a54625] + JET v0.9.9\n    Updating `~/compecon/ECON622/qmd/SomeNewPackage/test/Manifest.toml`\n  [da1fd8a2] + CodeTracking v1.3.6\n  [c3a54625] + JET v0.9.9\n  [aa1ae85d] + JuliaInterpreter v0.9.36\n  [6f1432cf] + LoweredCodeUtils v3.0.2\n  [1914dd2f] + MacroTools v0.5.13\n  [aea7be01] + PrecompileTools v1.2.1\n  [21216c6a] + Preferences v1.4.3\n  Activating project at `~/compecon/ECON622/qmd`\n  Activating new project at `~/compecon/ECON622/qmd/SomeNewPackage/docs`\n   Resolving package versions...\n   Installed OpenSSL_jll ─ v3.0.15+1\n    Updating `~/compecon/ECON622/qmd/SomeNewPackage/docs/Project.toml`\n  [e30172f5] + Documenter v1.7.0\n    Updating `~/compecon/ECON622/qmd/SomeNewPackage/docs/Manifest.toml`\n  [a4c015fc] + ANSIColoredPrinters v0.0.1\n  [1520ce14] + AbstractTrees v0.4.5\n  [944b1d66] + CodecZlib v0.7.6\n  [ffbed154] + DocStringExtensions v0.9.3\n  [e30172f5] + Documenter v1.7.0\n  [d7ba0133] + Git v1.3.1\n  [b5f81e59] + IOCapture v0.2.5\n  [692b3bcd] + JLLWrappers v1.6.0\n  [682c06a0] + JSON v0.21.4\n  [0e77f7df] + LazilyInitializedFields v1.2.2\n  [d0879d2d] + MarkdownAST v0.1.2\n  [69de0a69] + Parsers v2.8.1\n  [aea7be01] + PrecompileTools v1.2.1\n  [21216c6a] + Preferences v1.4.3\n  [2792f1a3] + RegistryInstances v0.1.0\n  [3bb67fe8] + TranscodingStreams v0.11.2\n  [2e619515] + Expat_jll v2.6.2+0\n  [f8c6e375] + Git_jll v2.44.0+2\n  [94ce4f54] + Libiconv_jll v1.17.0+0\n  [458c3c95] + OpenSSL_jll v3.0.15+1\n  [0dad84c5] + ArgTools v1.1.1\n  [56f22d72] + Artifacts\n  [2a0f44e3] + Base64\n  [ade2ca70] + Dates\n  [f43a241f] + Downloads v1.6.0\n  [7b1f6079] + FileWatching\n  [b77e0a4c] + InteractiveUtils\n  [b27032c2] + LibCURL v0.6.4\n  [76f85450] + LibGit2\n  [8f399da3] + Libdl\n  [56ddb016] + Logging\n  [d6f4376e] + Markdown\n  [a63ad114] + Mmap\n  [ca575930] + NetworkOptions v1.2.0\n  [44cfe95a] + Pkg v1.10.0\n  [de0858da] + Printf\n  [3fa0cd96] + REPL\n  [9a3f8284] + Random\n  [ea8e919c] + SHA v0.7.0\n  [9e88b42a] + Serialization\n  [6462fe0b] + Sockets\n  [fa267f1f] + TOML v1.0.3\n  [a4e569a6] + Tar v1.10.0\n  [8dfed614] + Test\n  [cf7118a7] + UUIDs\n  [4ec0a83e] + Unicode\n  [deac9b47] + LibCURL_jll v8.4.0+0\n  [e37daf67] + LibGit2_jll v1.6.4+0\n  [29816b5a] + LibSSH2_jll v1.11.0+1\n  [c8ffd9c3] + MbedTLS_jll v2.28.2+1\n  [14a3606d] + MozillaCACerts_jll v2023.1.10\n  [efcefdf7] + PCRE2_jll v10.42.0+1\n  [83775a58] + Zlib_jll v1.2.13+1\n  [8e850ede] + nghttp2_jll v1.52.0+1\n  [3f19e933] + p7zip_jll v17.4.0+2\nPrecompiling project...\n  ✓ OpenSSL_jll\n  ✓ Git_jll\n  ✓ Git\n  ✓ Documenter\n  4 dependencies successfully precompiled in 14 seconds. 19 already precompiled.\n   Resolving package versions...\n    Updating `~/compecon/ECON622/qmd/SomeNewPackage/docs/Project.toml`\n  [eade842b] + SomeNewPackage v1.0.0-DEV `..`\n    Updating `~/compecon/ECON622/qmd/SomeNewPackage/docs/Manifest.toml`\n  [eade842b] + SomeNewPackage v1.0.0-DEV `..`\n  Activating project at `~/compecon/ECON622/qmd`\n[ Info: Running posthooks\n[ Info: New package is at /home/paul/compecon/ECON622/qmd/SomeNewPackage\n\n\n\"/home/paul/compecon/ECON622/qmd/SomeNewPackage\""
  },
  {
    "objectID": "bestpractices.html#bestietemplate.jl",
    "href": "bestpractices.html#bestietemplate.jl",
    "title": "Best Practices",
    "section": "BestieTemplate.jl",
    "text": "BestieTemplate.jl\n\nusing BestieTemplate\nBestieTemplate.generate(\"TestPackage\")"
  },
  {
    "objectID": "bestpractices.html#tests",
    "href": "bestpractices.html#tests",
    "title": "Best Practices",
    "section": "Tests",
    "text": "Tests\n\nessential\norganize code into small functions, test them all\ntest both your code and code from others that you rely on"
  },
  {
    "objectID": "bestpractices.html#documentation",
    "href": "bestpractices.html#documentation",
    "title": "Best Practices",
    "section": "Documentation",
    "text": "Documentation\n\ncreate it for others and your future self"
  },
  {
    "objectID": "bestpractices.html#continuous-integration",
    "href": "bestpractices.html#continuous-integration",
    "title": "Best Practices",
    "section": "Continuous Integration",
    "text": "Continuous Integration\n\nAutomatically execute some actions in the cloud after git commits or pull requests or merges\n\nRun tests\nBuild documentation\nRun static code analysis / Linter\nCheck test coverage\netc\n\nMany providers\n\nGitHub Actions\nTravisCI\netc"
  },
  {
    "objectID": "bestpractices.html#test-coverage",
    "href": "bestpractices.html#test-coverage",
    "title": "Best Practices",
    "section": "Test Coverage",
    "text": "Test Coverage\n\nAutomatically try to determine which lines of code were executed during testing and produce a summary and report\napp.codecov.io\ncoveralls.io"
  },
  {
    "objectID": "bestpractices.html#static-code-analysis",
    "href": "bestpractices.html#static-code-analysis",
    "title": "Best Practices",
    "section": "Static Code Analysis",
    "text": "Static Code Analysis\n\n“linters” analyze code to detect errors and possible bugs\n\nbuilt into VSCode and other editors\nJET.jl for detecting type stability problems (advanced)\n\nformatters check for following text formatting standards around indentation and such"
  },
  {
    "objectID": "bestpractices.html#further-reading",
    "href": "bestpractices.html#further-reading",
    "title": "Best Practices",
    "section": "Further Reading",
    "text": "Further Reading\n\nGentzkow and Shapiro (2014)\nPruim, Gîrjău, and Horton (2023)\nWilson (2017)\nHaider, Riesch, and Jirauschek (2021)\nOrozco et al. (2020)"
  },
  {
    "objectID": "bestpractices.html#references",
    "href": "bestpractices.html#references",
    "title": "Best Practices",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nGentzkow, Matthew, and Jesse M. Shapiro. 2014. “Code and Data for the Social Sciences: A Practitioner’s Guide.” In. https://api.semanticscholar.org/CorpusID:62408223.\n\n\nHaider, Michael, Michael Riesch, and Christian Jirauschek. 2021. “Realization of Best Practices in Software Engineering and Scientific Writing Through Ready-to-Use Project Skeletons.” Optical and Quantum Electronics 53 (10): 568. https://doi.org/10.1007/s11082-021-03192-4.\n\n\nOrozco, Valérie, Christophe Bontemps, Elise Maigné, Virginie Piguet, Annie Hofstetter, Anne Lacroix, Fabrice Levert, and Jean-Marc Rousselle. 2020. “HOW TO MAKE a PIE: REPRODUCIBLE RESEARCH FOR EMPIRICAL ECONOMICS AND ECONOMETRICS.” Journal of Economic Surveys 34 (5): 1134–69. https://doi.org/https://doi.org/10.1111/joes.12389.\n\n\nPruim, Randall J., Maria-Cristiana Gîrjău, and Nicholas Jon Horton. 2023. “Fostering Better Coding Practices for Data Scientists.” https://arxiv.org/abs/2210.03991.\n\n\nWilson, Jennifer AND Cranston, Greg AND Bryan. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): 1–20. https://doi.org/10.1371/journal.pcbi.1005510."
  },
  {
    "objectID": "autodiff.html#derivatives",
    "href": "autodiff.html#derivatives",
    "title": "Automatic Differentiation",
    "section": "Derivatives",
    "text": "Derivatives\n\nNeeded for efficient equation solving and optimization\nCan calculate automatically"
  },
  {
    "objectID": "autodiff.html#finite-differences",
    "href": "autodiff.html#finite-differences",
    "title": "Automatic Differentiation",
    "section": "Finite Differences",
    "text": "Finite Differences\n\nf(x) = sin(x)/(1.0+exp(x))\n\nfunction dxfin(f,x)\n  h = sqrt(eps(x))\n  if abs(x) &gt; 1\n    h = h*abs(x)\n  end\n  (f(x+h) - f(x) )/ h\nend\n\ndxfin(f, 2.0)\n\n-0.1450763155960872"
  },
  {
    "objectID": "autodiff.html#forward-automatic-differentiation",
    "href": "autodiff.html#forward-automatic-differentiation",
    "title": "Automatic Differentiation",
    "section": "Forward Automatic Differentiation",
    "text": "Forward Automatic Differentiation\n\nmodule Forward\n\nstruct Dual{T}\n  v::T\n  dv::T\nend\n\nDual(x::T) where {T} = Dual(x, one(x))\n\nimport Base: +, sin, exp, *, /\n\nfunction (+)(a::T, x::Dual{T}) where {T}\n  Dual(a+x.v, x.dv)\nend\n\nfunction (*)(y::Dual, x::Dual)\n  Dual(y.v*x.v, x.v*y.dv + x.dv*y.v)\nend\n\nfunction (/)(x::Dual, y::Dual)\n  Dual(x.v/y.v, x.dv/y.v - x.v*y.dv/y.v^2)\nend\n\nexp(x::Dual) = Dual(exp(x.v), exp(x.v)*x.dv)\nsin(x::Dual) = Dual(sin(x.v), cos(x.v)*x.dv)\n\n\nfunction fdx(f,x)\n  out=f(Dual(x))\n  (out.v, out.dv)\nend\n\nend\n\nForward.fdx(f,2.0)\n\n(0.10839091026481387, -0.14507631594729084)"
  },
  {
    "objectID": "autodiff.html#reverse-automatic-differentiation",
    "href": "autodiff.html#reverse-automatic-differentiation",
    "title": "Automatic Differentiation",
    "section": "Reverse Automatic Differentiation",
    "text": "Reverse Automatic Differentiation\n\ncompute \\(f(x)\\) in usual forward direction, keep track of each operation and intermediate value\ncompute derivative “backwards”\n\n\\(f(x) = g(h(x))\\)\n\\(f'(x) = g'(h(x)) h'(x)\\)\n\nscales better for high dimensional \\(x\\)\nimplementation more complicated\n\nSimple-ish example https://simeonschaub.github.io/ReverseModePluto/notebook.html"
  },
  {
    "objectID": "autodiff.html#forwarddiff",
    "href": "autodiff.html#forwarddiff",
    "title": "Automatic Differentiation",
    "section": "ForwardDiff",
    "text": "ForwardDiff\n\nForwardDiff.jl\nmature and reliable"
  },
  {
    "objectID": "autodiff.html#forwarddiff-example",
    "href": "autodiff.html#forwarddiff-example",
    "title": "Automatic Differentiation",
    "section": "ForwardDiff Example",
    "text": "ForwardDiff Example\n\nusing Distributions\nfunction simulate_logit(observations, β)\n  x = randn(observations, length(β))\n  y = (x*β + rand(Logistic(), observations)) .&gt;= 0.0\n  return((y=y,x=x))\nend\n\nfunction logit_likelihood(β,y,x)\n  p = map(xb -&gt; cdf(Logistic(),xb), x*β)\n  sum(log.(ifelse.(y, p, 1.0 .- p)))\nend\n\nn = 500\nk = 3\nβ0 = ones(k)\n(y,x) = simulate_logit(n,β0)\n\nimport ForwardDiff\n∇L = ForwardDiff.gradient(b-&gt;logit_likelihood(b,y,x),β0)\n\n3-element Vector{Float64}:\n -8.401021936193848\n -2.5793470602763358\n  8.750532340154054"
  },
  {
    "objectID": "autodiff.html#forwarddiff-notes",
    "href": "autodiff.html#forwarddiff-notes",
    "title": "Automatic Differentiation",
    "section": "ForwardDiff Notes",
    "text": "ForwardDiff Notes\n\nFor \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), the computation scales with \\(n\\)\n\nbest for moderate \\(n\\)\n\nCode must be generic\n\nbe careful when allocating arrays\n\n\n\nfunction wontwork(x)\n  y = zeros(eltype(x),size(x))\n  for i ∈ eachindex(x)\n    y[i] += x[i]*i\n  end\n  return(sum(y))\nend\n\nfunction willwork(x)\n  y = zero(x)\n  for i ∈ eachindex(x)\n    y[i] += x[i]*i\n  end\n  return(sum(y))\nend\n\nbetterstyle(x) = sum(v*i for (i,v) in enumerate(x))"
  },
  {
    "objectID": "autodiff.html#zygote",
    "href": "autodiff.html#zygote",
    "title": "Automatic Differentiation",
    "section": "Zygote",
    "text": "Zygote\n\nZygote.jl\nDoes not allow mutating arrays\nQuite mature, but possibly some bugs remain\nApparently hard to develop, unclear future"
  },
  {
    "objectID": "autodiff.html#zygote-example",
    "href": "autodiff.html#zygote-example",
    "title": "Automatic Differentiation",
    "section": "Zygote Example",
    "text": "Zygote Example\n\nimport Zygote\nusing LinearAlgebra\n@time ∇Lz =  Zygote.gradient(b-&gt;logit_likelihood(b,y,x),β0)[1]\nnorm(∇L - ∇Lz)\n\n  0.298501 seconds (652.62 k allocations: 40.078 MiB, 4.77% gc time, 99.82% compilation time)\n\n\n3.2023728339893768e-15"
  },
  {
    "objectID": "autodiff.html#enzyme",
    "href": "autodiff.html#enzyme",
    "title": "Automatic Differentiation",
    "section": "Enzyme",
    "text": "Enzyme\n“Enzyme performs automatic differentiation (AD) of statically analyzable LLVM. It is highly-efficient and its ability to perform AD on optimized code allows Enzyme to meet or exceed the performance of state-of-the-art AD tools.”\n\nimport Enzyme\nimport Enzyme: Active, Duplicated, Const\n\ndb = zero(β0)\n@time Enzyme.autodiff(Enzyme.ReverseWithPrimal,logit_likelihood, Active, Duplicated(β0,db), Const(y), Const(x))\ndb\n\n┌ Warning: Using fallback BLAS replacements for ([\"dsymv_64_\"]), performance may be degraded\n└ @ Enzyme.Compiler ~/.julia/packages/GPUCompiler/y4cj1/src/utils.jl:59\n  1.522911 seconds (2.35 M allocations: 143.470 MiB, 2.44% gc time, 99.99% compilation time)\n\n\n3-element Vector{Float64}:\n -8.401021936193835\n -2.5793470602763398\n  8.750532340154058"
  },
  {
    "objectID": "autodiff.html#enzyme-notes",
    "href": "autodiff.html#enzyme-notes",
    "title": "Automatic Differentiation",
    "section": "Enzyme Notes",
    "text": "Enzyme Notes\n\nDocumentation is not suited to beginners\nDoes not work on all Julia code, but cases where it fails are not well documented. Calling Enzyme.API.runtimeActivity!(true) works around some errors.\nCryptic error messages. Enzyme operates on LLVM IR, and error messages often reference the point in the LLVM IR where the error occurred. Figuring out what Julia code the LLVM IR corresponds to is not easy.\n\nThese may be better now than last year when I first wrote this slide\n\n\n\nEnzyme.API.runtimeActivity!(false)\nf1(a,b) = sum(a.*b)\ndima = 30000\na = ones(dima)\nb = rand(dima)\nda = zeros(dima)\n@time Enzyme.autodiff(Enzyme.ReverseWithPrimal, f1, Duplicated(a,da),Const(b))\nda\n\nf3(a,b) = sum(a[i]*b[i] for i ∈ eachindex(a))\nda = zeros(dima)\nEnzyme.autodiff(Enzyme.ReverseWithPrimal, f3, Duplicated(a,da),Const(b))\nda\n\nif (false) # will trigger enzyme error without runtimeactivity\n  f2(a,b) = sum(a*b for (a,b) ∈ zip(a,b))\n  da = zeros(dima)\n  @time Enzyme.autodiff(Enzyme.ReverseWithPrimal, f2, Duplicated(a,da), Const(b))\n  da\nend\n\nEnzyme.API.runtimeActivity!(true)\nf2(a,b) = sum(a*b for (a,b) ∈ zip(a,b))\nda = zeros(dima)\n@time Enzyme.autodiff(Enzyme.ReverseWithPrimal, f2, Duplicated(a,da), Const(b))\nda\n\n  1.076050 seconds (1.72 M allocations: 116.872 MiB, 2.47% gc time, 99.13% compilation time)\n  0.284568 seconds (1.58 M allocations: 115.268 MiB, 5.50% gc time, 99.89% compilation time)\n\n\n30000-element Vector{Float64}:\n 0.9285498547482849\n 0.4818033538222657\n 0.4285829345208757\n 0.11249405663627943\n 0.6815029880057336\n 0.7759668486381841\n 0.27710153082937505\n 0.9955811391617984\n 0.9179127296555908\n 0.08558892746967195\n ⋮\n 0.7459199045525703\n 0.2513807047428307\n 0.9481477587080472\n 0.02671943248956099\n 0.00743381088813222\n 0.20424581522508045\n 0.797822379770914\n 0.4752628647520214\n 0.8273610538041952"
  },
  {
    "objectID": "autodiff.html#finitediff",
    "href": "autodiff.html#finitediff",
    "title": "Automatic Differentiation",
    "section": "FiniteDiff",
    "text": "FiniteDiff\n\nFiniteDiff computes finite difference gradients– always test that whatever automatic or manual derivatives you compute are close to the finite difference versions\nuse a package for finite differences to handle rounding error well"
  },
  {
    "objectID": "autodiff.html#chainrules",
    "href": "autodiff.html#chainrules",
    "title": "Automatic Differentiation",
    "section": "ChainRules",
    "text": "ChainRules\n\nChainRules\nused by many AD packages to define the derivatives of various functions.\nUseful if you want to define a custom derivative rule for a function."
  },
  {
    "objectID": "autodiff.html#differentiationinterface",
    "href": "autodiff.html#differentiationinterface",
    "title": "Automatic Differentiation",
    "section": "DifferentiationInterface",
    "text": "DifferentiationInterface\n\nDifferentiationInterface gives a single interface for many differentiation packages\n\n\nimport DifferentiationInterface as DI\nDI.gradient(b-&gt;logit_likelihood(b,y,x), DI.AutoEnzyme(),β0)\n\n┌ Warning: Using fallback BLAS replacements for ([\"dsymv_64_\"]), performance may be degraded\n└ @ Enzyme.Compiler ~/.julia/packages/GPUCompiler/y4cj1/src/utils.jl:59\n\n\n3-element Vector{Float64}:\n -8.401021936193835\n -2.5793470602763398\n  8.750532340154058\n\n\n\nimprove performance by reusing intermediate variables\n\n\nbackend = DI.AutoEnzyme()\ndcache = DI.prepare_gradient(b-&gt;logit_likelihood(b,y,x), backend, β0)\ngrad = zero(β0)\nDI.gradient!(b-&gt;logit_likelihood(b,y,x),grad, backend,β0 , dcache)\n\n3-element Vector{Float64}:\n -8.401021936193835\n -2.5793470602763398\n  8.750532340154058"
  },
  {
    "objectID": "autodiff.html#other-packages-1",
    "href": "autodiff.html#other-packages-1",
    "title": "Automatic Differentiation",
    "section": "Other Packages",
    "text": "Other Packages\n\nhttps://juliadiff.org/"
  },
  {
    "objectID": "autodiff.html#reversediff.jl",
    "href": "autodiff.html#reversediff.jl",
    "title": "Automatic Differentiation",
    "section": "ReverseDiff.jl",
    "text": "ReverseDiff.jl\n\nReverseDiff.jl a tape based reverse mode package\nLong lived and well tested\nlimitations. Importantly, code must be generic and mutation of arrays is not allowed."
  },
  {
    "objectID": "autodiff.html#yota.jl",
    "href": "autodiff.html#yota.jl",
    "title": "Automatic Differentiation",
    "section": "Yota.jl",
    "text": "Yota.jl\n\nYota.jl another tape based package\nCompatible with Chainrules.jl\nSomewhat newer and less popular\nIts documentation has a very nice explanation of how it works."
  },
  {
    "objectID": "autodiff.html#tracker",
    "href": "autodiff.html#tracker",
    "title": "Automatic Differentiation",
    "section": "Tracker",
    "text": "Tracker\nTracker is a tape based reverse mode package. It was the default autodiff package in Flux before being replaced by Zygote. No longer under active development."
  },
  {
    "objectID": "autodiff.html#diffractor",
    "href": "autodiff.html#diffractor",
    "title": "Automatic Differentiation",
    "section": "Diffractor",
    "text": "Diffractor\nDiffractor is automatic differentiation package in development. It was once hoped to be the future of AD in Julia, but has been delayed. It plans to have both forward and reverse mode, but only forward mode is available so far."
  },
  {
    "objectID": "optimization.html#optimization-1",
    "href": "optimization.html#optimization-1",
    "title": "Optimization",
    "section": "Optimization",
    "text": "Optimization\n\\[\n\\max_{x} f(x)\n\\]"
  },
  {
    "objectID": "optimization.html#overview-of-algorithms",
    "href": "optimization.html#overview-of-algorithms",
    "title": "Optimization",
    "section": "Overview of Algorithms",
    "text": "Overview of Algorithms\n\nhttps://schrimpf.github.io/AnimatedOptimization.jl/optimization/"
  },
  {
    "objectID": "optimization.html#example-problem",
    "href": "optimization.html#example-problem",
    "title": "Optimization",
    "section": "Example Problem",
    "text": "Example Problem\n\n\nCode\nusing Statistics, LinearAlgebra\n\nfunction cueiv(β; n=1000, σ=0.1, γ=[I; ones(2,length(β))], ρ=0.5)\n  z = randn(n, size(γ)[1])\n  endo = randn(n, length(β))\n  x = z*γ .+ endo\n  ϵ = σ*(randn(n)*sqrt(1.0-ρ^2).+endo[:,1]*ρ)\n  y =  x*β .+ ϵ\n  g(β) = (y - x*β).*z\n  function cueobj(β)\n    G = g(β)\n    Eg=mean(G,dims=1)\n    W = inv(cov(G))\n    (n*Eg*W*Eg')[1]\n  end\n  return(cueobj, x, y, z)\nend\n\n\ncueiv (generic function with 1 method)\n\n\n\n\nCode\nusing Plots\nβ = ones(2)\nf, x, y ,z = cueiv(β;σ=0.1,ρ=0.9)\nb1 = range(0,2,length=100)\nb2 = range(0,2,length=100)\nfval = ((x,y)-&gt;f([x,y])).(b1,b2')\nPlots.plotly()\ncontourf(b1,b2,fval)\n\n\n┌ Warning: Failed to load integration with PlotlyBase & PlotlyKaleido.\n│   exception =\n│    ArgumentError: Package PlotlyKaleido not found in current path.\n│    - Run `import Pkg; Pkg.add(\"PlotlyKaleido\")` to install the PlotlyKaleido package.\n│    Stacktrace:\n│      [1] macro expansion\n│        @ ./loading.jl:1772 [inlined]\n│      [2] macro expansion\n│        @ ./lock.jl:267 [inlined]\n│      [3] __require(into::Module, mod::Symbol)\n│        @ Base ./loading.jl:1753\n│      [4] #invoke_in_world#3\n│        @ ./essentials.jl:926 [inlined]\n│      [5] invoke_in_world\n│        @ ./essentials.jl:923 [inlined]\n│      [6] require(into::Module, mod::Symbol)\n│        @ Base ./loading.jl:1746\n│      [7] top-level scope\n│        @ ~/.julia/packages/Plots/kLeqV/src/backends.jl:569\n│      [8] eval\n│        @ ./boot.jl:385 [inlined]\n│      [9] _initialize_backend(pkg::Plots.PlotlyBackend)\n│        @ Plots ~/.julia/packages/Plots/kLeqV/src/backends.jl:567\n│     [10] backend\n│        @ ~/.julia/packages/Plots/kLeqV/src/backends.jl:245 [inlined]\n│     [11] plotly()\n│        @ Plots ~/.julia/packages/Plots/kLeqV/src/backends.jl:86\n│     [12] top-level scope\n│        @ ~/compecon/ECON622/qmd/optimization.qmd:69\n│     [13] eval\n│        @ ./boot.jl:385 [inlined]\n│     [14] (::QuartoNotebookWorker.var\"#17#18\"{Module, Expr})()\n│        @ QuartoNotebookWorker ~/.julia/packages/QuartoNotebookRunner/VUFgu/src/QuartoNotebookWorker/src/render.jl:219\n│     [15] (::QuartoNotebookWorker.Packages.IOCapture.var\"#5#9\"{DataType, QuartoNotebookWorker.var\"#17#18\"{Module, Expr}, IOContext{Base.PipeEndpoint}, IOContext{Base.PipeEndpoint}, IOContext{IOStream}, IOContext{Base.PipeEndpoint}})()\n│        @ QuartoNotebookWorker.Packages.IOCapture ~/.julia/packages/IOCapture/Y5rEA/src/IOCapture.jl:170\n│     [16] with_logstate(f::Function, logstate::Any)\n│        @ Base.CoreLogging ./logging.jl:515\n│     [17] with_logger\n│        @ ./logging.jl:627 [inlined]\n│     [18] capture(f::QuartoNotebookWorker.var\"#17#18\"{Module, Expr}; rethrow::Type, color::Bool, passthrough::Bool, capture_buffer::IOBuffer, io_context::Vector{Pair{Symbol, Any}})\n│        @ QuartoNotebookWorker.Packages.IOCapture ~/.julia/packages/IOCapture/Y5rEA/src/IOCapture.jl:167\n│     [19] include_str(mod::Module, code::String; file::String, line::Int64, cell_options::Dict{String, Any})\n│        @ QuartoNotebookWorker ~/.julia/packages/QuartoNotebookRunner/VUFgu/src/QuartoNotebookWorker/src/render.jl:199\n│     [20] #invokelatest#2\n│        @ ./essentials.jl:894 [inlined]\n│     [21] invokelatest\n│        @ ./essentials.jl:889 [inlined]\n│     [22] #6\n│        @ ~/.julia/packages/QuartoNotebookRunner/VUFgu/src/QuartoNotebookWorker/src/render.jl:18 [inlined]\n│     [23] with_inline_display(f::QuartoNotebookWorker.var\"#6#7\"{String, String, Int64, Dict{String, Any}}, cell_options::Dict{String, Any})\n│        @ QuartoNotebookWorker ~/.julia/packages/QuartoNotebookRunner/VUFgu/src/QuartoNotebookWorker/src/InlineDisplay.jl:26\n│     [24] _render_thunk(thunk::Function, code::String, cell_options::Dict{String, Any}, is_expansion_ref::Base.RefValue{Bool}; inline::Bool)\n│        @ QuartoNotebookWorker ~/.julia/packages/QuartoNotebookRunner/VUFgu/src/QuartoNotebookWorker/src/render.jl:42\n│     [25] _render_thunk\n│        @ ~/.julia/packages/QuartoNotebookRunner/VUFgu/src/QuartoNotebookWorker/src/render.jl:35 [inlined]\n│     [26] #render#5\n│        @ ~/.julia/packages/QuartoNotebookRunner/VUFgu/src/QuartoNotebookWorker/src/render.jl:15 [inlined]\n│     [27] render(code::String, file::String, line::Int64, cell_options::Dict{String, Any})\n│        @ QuartoNotebookWorker ~/.julia/packages/QuartoNotebookRunner/VUFgu/src/QuartoNotebookWorker/src/render.jl:1\n│     [28] render(::String, ::Vararg{Any}; kwargs::@Kwargs{})\n│        @ Main ~/.julia/packages/QuartoNotebookRunner/VUFgu/src/worker.jl:14\n│     [29] top-level scope\n│        @ none:1\n│     [30] eval\n│        @ ./boot.jl:385 [inlined]\n│     [31] (::Main.var\"#1#2\"{Sockets.TCPSocket, UInt64, Bool, @Kwargs{}, Tuple{Module, Expr}, typeof(Core.eval)})()\n│        @ Main ~/.julia/packages/Malt/Z3YQq/src/worker.jl:120\n└ @ Plots ~/.julia/packages/Plots/kLeqV/src/backends.jl:577"
  },
  {
    "objectID": "optimization.html#optim.jl",
    "href": "optimization.html#optim.jl",
    "title": "Optimization",
    "section": "Optim.jl",
    "text": "Optim.jl\n\nPure Julia implementation of standard set of optimization algorithms. This is generally a good starting point, especially for unconstrained nonlinear problems without too many variables (maybe up to a few hundred).\n\n\nOptim docs\n\n\n\nCode\nimport Optim\nβ = ones(2)\nf = cueiv(β;σ=0.1,ρ=0.5)[1]\nβ0 = zero(β)\n\nsol=Optim.optimize(f, β0, Optim.NelderMead())\nsol=Optim.optimize(f,β0, Optim.LBFGS(), autodiff=:forward)\nsol=Optim.optimize(f,β0, Optim.LBFGS(m=20, linesearch=Optim.LineSearches.BackTracking()), autodiff=:forward)\nsol=Optim.optimize(f,β0,Optim.NewtonTrustRegion(), autodiff=:forward)\n\nβ = ones(20)\nf =  cueiv(β)[1]\nβ0 = zero(β)\nsol=Optim.optimize(f, β0, Optim.NelderMead())\nsol=Optim.optimize(f,β0, Optim.LBFGS(), autodiff=:forward)\nsol=Optim.optimize(f,β0, Optim.LBFGS(m=20, linesearch=Optim.LineSearches.BackTracking()), autodiff=:forward)\nsol=Optim.optimize(f,β0,Optim.NewtonTrustRegion(), autodiff=:forward)\n\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     1.925800e+02\n\n * Found with\n    Algorithm:     Newton's Method (Trust Region)\n\n * Convergence measures\n    |x - x'|               = 4.93e+01 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.08e-02 ≰ 0.0e+00\n    |f(x) - f(x')|         = 2.04e-06 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 1.06e-08 ≰ 0.0e+00\n    |g(x)|                 = 9.94e-09 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   28  (vs limit Inf)\n    Iterations:    115\n    f(x) calls:    116\n    ∇f(x) calls:   116\n    ∇²f(x) calls:  113"
  },
  {
    "objectID": "optimization.html#optimization.jl",
    "href": "optimization.html#optimization.jl",
    "title": "Optimization",
    "section": "Optimization.jl",
    "text": "Optimization.jl\n\nA package that provides a uniform interface to many other optimization packages. A good choice for easy experimentation and trying different packages.\n\n\nOptimization docs\nUnified interface to many packages (like NonlinearSolve.jl, but less polished)"
  },
  {
    "objectID": "optimization.html#juliasmoothoptimizers",
    "href": "optimization.html#juliasmoothoptimizers",
    "title": "Optimization",
    "section": "JuliaSmoothOptimizers",
    "text": "JuliaSmoothOptimizers\n\nA smaller set of developers and users than Optim, but has some good ideas and algorithms. Multiple options for nonlinear constrained optimization, some with promising benchmarks.. Design makes using specialized linear solvers inside nonlinear algorithms convenient.\n\n\nOrganization README\n\nusing ADNLPModels\nimport DCISolver, JSOSolvers, Percival\nβ = ones(2)\nf =  cueiv(β; γ = I + zeros(2,2))[1]\nβ0 = zero(β)\n\nnlp = ADNLPModel(f, β0)\nstats=JSOSolvers.lbfgs(nlp)\nprint(stats)\n\nstats=JSOSolvers.tron(nlp)\nprint(stats)\n\necnlp = ADNLPModel(f,β0, b-&gt;(length(β0) - sum(b)), zeros(1), zeros(1))\nstats=DCISolver.dci(ecnlp)\nprint(stats)\n\nicnlp = ADNLPModel(f,β0, b-&gt;(length(β0) - sum(b)), -ones(1), ones(1))\nstats=Percival.percival(icnlp)\nprint(stats)"
  },
  {
    "objectID": "optimization.html#jump",
    "href": "optimization.html#jump",
    "title": "Optimization",
    "section": "JuMP",
    "text": "JuMP\n\nJuMP is a modelling language for optimization in Julia\nmany solvers\nbest when write problem in special JuMP syntax\n\nefficiently calculate derivatives and recognize special problem structure such as linearity, sparsity, etc.\n\n\n\n\nCode\nusing JuMP\nimport MadNLP, Ipopt\nk = 10\nβ = ones(k)\nf,x,y,z =  cueiv(β; γ = I + zeros(k,k))\nβ0 = zero(β)\nn,k = size(x)\n\n# sub-optimal usage of JuMP\nm = Model(()-&gt;MadNLP.Optimizer(print_level=MadNLP.INFO, max_iter=100))\n@variable(m, β[1:k])\n@operator(m, op_f, k, (x...)-&gt;f(vcat(x...))) # hides internals of f from JuMP\n@objective(m, Min, op_f(β...))\noptimize!(m)\nJuMP.value.(β)\n\n\n┌ Warning: Replacing docs for `SolverCore.solve! :: Union{}` in module `MadNLP`\n└ @ Base.Docs docs/Docs.jl:243\nThis is MadNLP version v0.8.4, running with umfpack\n\nNumber of nonzeros in constraint Jacobian............:        0\nNumber of nonzeros in Lagrangian Hessian.............:        0\n\nTotal number of variables............................:       10\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  3.4959747e+02 0.00e+00 3.18e+01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n   1  3.4275708e+02 0.00e+00 2.30e+01  -1.0 9.55e+01    -  1.00e+00 3.12e-02f  6\n   2  3.0505724e+02 0.00e+00 1.46e+01  -1.0 9.01e-01    -  1.00e+00 1.00e+00f  1\n   3  2.7079028e+02 0.00e+00 1.28e+01  -1.0 2.76e+00    -  1.00e+00 1.00e+00f  1\n   4  2.3816600e+02 0.00e+00 7.48e+00  -1.0 1.75e+00    -  1.00e+00 1.00e+00f  1\n   5  2.1025967e+02 0.00e+00 6.09e+00  -1.0 4.38e+00    -  1.00e+00 1.00e+00f  1\n   6  2.0408235e+02 0.00e+00 1.53e+00  -1.0 2.87e+00    -  1.00e+00 5.00e-01f  2\n   7  2.0282469e+02 0.00e+00 9.75e-01  -1.0 5.62e-01    -  1.00e+00 1.00e+00f  1\n   8  2.0218956e+02 0.00e+00 7.06e-01  -1.7 4.74e-01    -  1.00e+00 1.00e+00h  1\n   9  2.0183708e+02 0.00e+00 6.60e-01  -1.7 8.32e-01    -  1.00e+00 1.00e+00h  1\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  10  2.0165613e+02 0.00e+00 2.50e-01  -1.7 2.94e-01    -  1.00e+00 1.00e+00h  1\n  11  2.0163690e+02 0.00e+00 3.78e-02  -1.7 8.30e-02    -  1.00e+00 1.00e+00h  1\n  12  2.0163549e+02 0.00e+00 1.66e-02  -2.5 3.03e-02    -  1.00e+00 1.00e+00h  1\n  13  2.0163497e+02 0.00e+00 1.74e-02  -3.8 2.80e-02    -  1.00e+00 1.00e+00h  1\n  14  2.0163465e+02 0.00e+00 2.12e-02  -3.8 6.11e-02    -  1.00e+00 5.00e-01h  2\n  15  2.0163363e+02 0.00e+00 3.13e-02  -3.8 6.35e-02    -  1.00e+00 1.00e+00h  1\n  16  2.0162529e+02 0.00e+00 1.38e-01  -3.8 2.92e+00    -  1.00e+00 2.50e-01h  3\n  17  2.0161175e+02 0.00e+00 3.45e-01  -3.8 8.80e+01    -  1.00e+00 1.56e-02f  7\n  18  2.0160805e+02 0.00e+00 2.65e-01  -3.8 1.05e+02    -  1.00e+00 3.91e-03f  9\n  19  2.0160487e+02 0.00e+00 2.96e-01  -3.8 1.81e+01    -  1.00e+00 1.56e-02h  7\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  20  2.0154197e+02 0.00e+00 1.57e+00  -3.8 1.25e+02    -  1.00e+00 3.12e-02f  6\n  21  2.0154053e+02 0.00e+00 1.58e+00  -3.8 3.34e+02    -  1.00e+00 4.88e-04f  12\n  22  2.0153986e+02 0.00e+00 1.98e+00  -3.8 6.29e+01    -  1.00e+00 7.81e-03f  8\n  23  2.0153471e+02 0.00e+00 2.84e+00  -3.8 1.57e+02    -  1.00e+00 1.95e-03f  10\n  24  2.0153063e+02 0.00e+00 3.14e+00  -3.8 1.11e+02    -  1.00e+00 1.95e-03f  10\n  25  2.0152695e+02 0.00e+00 3.37e+00  -3.8 2.89e+01    -  1.00e+00 3.91e-03f  9\n  26  2.0152681e+02 0.00e+00 7.14e+00  -3.8 1.37e+01    -  1.00e+00 6.25e-02f  5\n  27  2.0150544e+02 0.00e+00 8.95e+00  -3.8 5.10e+02    -  1.00e+00 4.88e-04f  12\n  28  2.0149840e+02 0.00e+00 1.11e+01  -3.8 8.14e+01    -  1.00e+00 1.95e-03f  10\n  29  2.0148435e+02 0.00e+00 1.43e+01  -3.8 4.83e+01    -  1.00e+00 3.91e-03f  9\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  30  2.0146691e+02 0.00e+00 1.77e+01  -3.8 5.70e+01    -  1.00e+00 1.95e-03f  10\n  31  2.0146042e+02 0.00e+00 2.65e+01  -3.8 4.39e+01    -  1.00e+00 3.91e-03f  9\n  32  2.0145505e+02 0.00e+00 2.79e+01  -3.8 1.61e+01    -  1.00e+00 3.91e-03f  9\n  33  2.0142697e+02 0.00e+00 3.27e+01  -3.8 4.23e+01    -  1.00e+00 1.95e-03f  10\n  34  2.0134141e+02 0.00e+00 4.76e+01  -3.8 1.28e+02    -  1.00e+00 9.77e-04f  11\n  35  2.0131932e+02 0.00e+00 7.14e+01  -3.8 1.23e+02    -  1.00e+00 9.77e-04f  11\n  36  2.0129103e+02 0.00e+00 9.40e+01  -3.8 3.12e+01    -  1.00e+00 1.95e-03f  10\n  37  2.0127522e+02 0.00e+00 1.50e+02  -3.8 2.53e+01    -  1.00e+00 3.91e-03f  9\n  38  2.0123251e+02 0.00e+00 1.72e+02  -3.8 3.04e+01    -  1.00e+00 9.77e-04f  11\n  39  2.0002288e+02 0.00e+00 3.75e+02  -3.8 2.47e+01    -  1.00e+00 3.91e-03f  9\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  40  1.9736971e+02 0.00e+00 6.32e+02  -3.8 4.02e+02    -  1.00e+00 1.22e-04f  14\n  41  1.9716351e+02 0.00e+00 7.10e+02  -3.8 1.46e+01    -  1.00e+00 4.88e-04f  12\n  42  1.9689903e+02 0.00e+00 7.34e+02  -3.8 8.41e-02    -  1.00e+00 3.12e-02f  6\n  43  1.9451806e+02 0.00e+00 6.09e+02  -3.8 3.68e-02    -  1.00e+00 5.00e-01f  2\n  44  1.9169873e+02 0.00e+00 7.76e+02  -3.8 2.93e-02    -  1.00e+00 1.00e+00f  1\n  45  1.7128022e+02 0.00e+00 1.21e+03  -3.8 7.14e-02    -  1.00e+00 5.00e-01f  2\n  46  1.6238472e+02 0.00e+00 1.51e+03  -3.8 5.23e-02    -  1.00e+00 1.00e+00f  1\n  47  1.4215950e+02 0.00e+00 7.00e+02  -3.8 3.85e-02    -  1.00e+00 1.00e+00f  1\n  48  8.6258499e+01 0.00e+00 2.69e+03  -3.8 4.42e-02    -  1.00e+00 1.00e+00f  1\n  49  5.6918124e+01 0.00e+00 2.54e+03  -3.8 5.08e-02    -  1.00e+00 2.50e-01f  3\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  50  2.8932737e+01 0.00e+00 1.58e+03  -3.8 4.72e-02    -  1.00e+00 5.00e-01f  2\n  51  2.9319167e+00 0.00e+00 6.21e+02  -3.8 8.41e-03    -  1.00e+00 1.00e+00f  1\n  52  6.2044148e-01 0.00e+00 2.61e+02  -3.8 5.42e-03    -  1.00e+00 5.00e-01f  2\n  53  7.8362428e-02 0.00e+00 1.11e+02  -3.8 1.80e-03    -  1.00e+00 1.00e+00h  1\n  54  1.6383381e-02 0.00e+00 8.09e+01  -3.8 6.76e-04    -  1.00e+00 1.00e+00h  1\n  55  1.4911326e-03 0.00e+00 1.51e+01  -3.8 3.93e-04    -  1.00e+00 1.00e+00h  1\n  56  4.5174273e-05 0.00e+00 2.35e+00  -3.8 6.10e-05    -  1.00e+00 1.00e+00h  1\n  57  3.0187950e-06 0.00e+00 6.10e-01  -3.8 1.32e-05    -  1.00e+00 1.00e+00h  1\n  58  7.2713672e-08 0.00e+00 1.28e-01  -3.8 2.86e-06    -  1.00e+00 1.00e+00h  1\n  59  1.4860218e-09 0.00e+00 1.86e-02  -3.8 7.46e-07    -  1.00e+00 1.00e+00h  1\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  60  3.1452161e-10 0.00e+00 6.85e-03  -3.8 1.15e-07    -  1.00e+00 1.00e+00h  1\n  61  2.5026853e-12 0.00e+00 7.91e-04  -3.8 2.61e-08    -  1.00e+00 1.00e+00h  1\n  62  9.9820837e-14 0.00e+00 1.29e-04  -5.7 2.89e-09    -  1.00e+00 1.00e+00h  1\n  63  7.0509290e-15 0.00e+00 3.11e-05  -5.7 5.54e-10    -  1.00e+00 1.00e+00h  1\n  64  7.7629621e-17 0.00e+00 4.03e-06  -5.7 2.05e-10    -  1.00e+00 1.00e+00h  1\n  65  1.3438193e-17 0.00e+00 1.82e-06  -8.6 2.27e-11    -  1.00e+00 1.00e+00h  1\n  66  9.6249111e-21 0.00e+00 3.70e-08  -8.6 7.33e-12    -  1.00e+00 1.00e+00h  1\n  67  5.1326728e-22 0.00e+00 5.44e-09  -8.6 1.59e-13    -  1.00e+00 1.00e+00h  1\n\nNumber of Iterations....: 67\n\n                                   (scaled)                 (unscaled)\nObjective...............:   5.1326727756299644e-22    5.1326727756299644e-22\nDual infeasibility......:   5.4439742191250211e-09    5.4439742191250211e-09\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   5.4439742191250211e-09    5.4439742191250211e-09\n\nNumber of objective function evaluations             = 442\nNumber of objective gradient evaluations             = 68\nNumber of constraint evaluations                     = 442\nNumber of constraint Jacobian evaluations            = 1\nNumber of Lagrangian Hessian evaluations             = 0\nTotal wall-clock secs in solver (w/o fun. eval./lin. alg.)  = 19.761\nTotal wall-clock secs in linear solver                      =  0.478\nTotal wall-clock secs in NLP function evaluations           =  0.255\nTotal wall-clock secs                                       = 20.494\n\nEXIT: Optimal Solution Found (tol = 1.0e-08).\n\n\n10-element Vector{Float64}:\n 1.0019790066553154\n 0.9972041660398521\n 0.9983436538794038\n 1.0038204311153165\n 0.9945321338917558\n 0.998154052014748\n 1.0039342503480513\n 1.0039944742522768\n 0.995661392828938\n 0.9993280224181564\n\n\n\n\nCode\n# better usage of JuMP\nm2 = Model(()-&gt;MadNLP.Optimizer(print_level=MadNLP.INFO, max_iter=100))\n#m2 = Model(Ipopt.Optimizer)\n#set_attribute(m2, \"print_level\", 5)\nn, k = size(x)\n@variable(m2, β2[1:k])\ng = (y - x*β2).*z\nEg=mean(g,dims=1)\ninvW = cov(g)\n@variable(m2, Wg[1:k])\n@constraint(m2, invW*Wg .== Eg')\n@objective(m2, Min, n*dot(Eg,Wg))\noptimize!(m2)\nJuMP.value.(β2)\n\n\nThis is MadNLP version v0.8.4, running with umfpack\n\nNumber of nonzeros in constraint Jacobian............:      200\nNumber of nonzeros in Lagrangian Hessian.............:     1750\n\nTotal number of variables............................:       20\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:       10\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  0.0000000e+00 1.30e+00 4.04e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n   1  3.6578937e+02 7.43e-01 1.56e+01  -1.0 6.25e-01    -  1.00e+00 1.00e+00h  1\n   2  2.9062703e+02 5.18e-03 2.39e+00  -1.0 2.42e-02   2.0 1.00e+00 1.00e+00h  1\n   3 -8.8949112e+00 3.10e+00 2.26e+02  -1.0 1.68e+00    -  1.00e+00 1.00e+00h  1\n   4  1.2385249e+00 1.04e+00 7.97e+01  -1.0 3.94e+00    -  1.00e+00 1.00e+00h  1\n   5 -3.2203872e-02 8.29e-03 6.32e-01  -1.0 9.60e-01    -  1.00e+00 1.00e+00h  1\n   6  6.2813486e-08 5.95e-06 4.69e-04  -1.7 9.45e-03    -  1.00e+00 1.00e+00h  1\n   7  2.9877420e-14 3.09e-12 7.05e-06  -5.7 6.90e-04    -  1.00e+00 1.00e+00h  1\n   8  4.8467614e-27 3.29e-16 4.10e-14  -8.6 4.77e-08    -  1.00e+00 1.00e+00h  1\n\nNumber of Iterations....: 8\n\n                                   (scaled)                 (unscaled)\nObjective...............:   3.7198411363219821e-28    4.8467614016778965e-27\nDual infeasibility......:   4.0998006929452738e-14    5.3418291332695970e-13\nConstraint violation....:   3.2864463628030602e-16    3.2864463628030602e-16\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   3.1465562268156216e-15    4.0998006929452738e-14\n\nNumber of objective function evaluations             = 9\nNumber of objective gradient evaluations             = 9\nNumber of constraint evaluations                     = 9\nNumber of constraint Jacobian evaluations            = 9\nNumber of Lagrangian Hessian evaluations             = 8\nTotal wall-clock secs in solver (w/o fun. eval./lin. alg.)  =  3.851\nTotal wall-clock secs in linear solver                      =  0.001\nTotal wall-clock secs in NLP function evaluations           =  0.452\nTotal wall-clock secs                                       =  4.304\n\nEXIT: Optimal Solution Found (tol = 1.0e-08).\n\n\n10-element Vector{Float64}:\n 1.0019790066553267\n 0.9972041660398772\n 0.9983436538793767\n 1.0038204311153398\n 0.9945321338917862\n 0.9981540520147151\n 1.0039342503480337\n 1.0039944742522884\n 0.9956613928289493\n 0.9993280224181971\n\n\n\n\nCode\n# or Empirical Likelihood\nm3 = Model(()-&gt;MadNLP.Optimizer(print_level=MadNLP.INFO, max_iter=100))\n@variable(m3, β3[1:k])\n@variable(m3, 1e-8 &lt;= p[1:n] &lt;= 1-1e-8)\n@constraint(m3, sum(p) == 1)\n@objective(m3, Max, 1/n*sum(log.(p)))\nJuMP.set_start_value.(p, 1/n)\ng3 = (y - x*β3).*z\n@constraint(m3, p'*g3 .== 0)\noptimize!(m3)\nJuMP.value.(β3)\n\n\nThis is MadNLP version v0.8.4, running with umfpack\n\nNumber of nonzeros in constraint Jacobian............:   211000\nNumber of nonzeros in Lagrangian Hessian.............:   101000\n\nTotal number of variables............................:     1010\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:     1000\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:       11\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  4.6051702e+00 1.30e+01 7.73e-15  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n   1  6.9077553e+00 1.17e+00 8.10e-01  -1.0 1.00e-01    -  1.00e+00 1.00e+00h  1\n   2  6.9077553e+00 1.87e-12 4.26e-14  -1.0 9.04e-01    -  1.00e+00 1.00e+00h  1\n   3  6.9077553e+00 1.39e-14 3.51e-14  -2.5 1.45e-12    -  1.00e+00 1.00e+00h  1\n   4  6.9077553e+00 3.84e-15 1.11e-15  -5.7 1.34e-14    -  1.00e+00 1.00e+00h  1\n   5  6.9077553e+00 3.58e-15 2.22e-16  -9.0 3.79e-15    -  1.00e+00 1.00e+00h  1\n\nNumber of Iterations....: 5\n\n                                   (scaled)                 (unscaled)\nObjective...............:   6.9077552789822088e+00    6.9077552789822088e+00\nDual infeasibility......:   2.2204460492503131e-16    2.2204460492503131e-16\nConstraint violation....:   3.5752650839881994e-15    3.5752650839881994e-15\nComplementarity.........:   9.0909090940771483e-10    9.0909090940771483e-10\nOverall NLP error.......:   9.0909090940771483e-10    9.0909090940771483e-10\n\nNumber of objective function evaluations             = 6\nNumber of objective gradient evaluations             = 6\nNumber of constraint evaluations                     = 6\nNumber of constraint Jacobian evaluations            = 6\nNumber of Lagrangian Hessian evaluations             = 5\nTotal wall-clock secs in solver (w/o fun. eval./lin. alg.)  =  0.022\nTotal wall-clock secs in linear solver                      =  0.011\nTotal wall-clock secs in NLP function evaluations           =  0.005\nTotal wall-clock secs                                       =  0.038\n\nEXIT: Optimal Solution Found (tol = 1.0e-08).\n\n\n10-element Vector{Float64}:\n 1.001979006655325\n 0.9972041660398774\n 0.9983436538793784\n 1.0038204311153387\n 0.9945321338917851\n 0.9981540520147166\n 1.0039342503480353\n 1.0039944742522933\n 0.9956613928289548\n 0.9993280224181954"
  },
  {
    "objectID": "optimization.html#ipopt",
    "href": "optimization.html#ipopt",
    "title": "Optimization",
    "section": "Ipopt",
    "text": "Ipopt\nAn open source solver written that works well on large constrained nonlinear problems. Can be used directly or through JuMP, Optimization.jl or JuliaSmoothOptimizers interfaces."
  },
  {
    "objectID": "optimization.html#madnlp",
    "href": "optimization.html#madnlp",
    "title": "Optimization",
    "section": "MadNLP",
    "text": "MadNLP\nInterior point solver written in Julia."
  },
  {
    "objectID": "optimization.html#knitro",
    "href": "optimization.html#knitro",
    "title": "Optimization",
    "section": "Knitro",
    "text": "Knitro\nA commercial solver with state of the art performance on many problems. Maybe worth the cost for large constrained nonlinear problems. Can be used directly or through JuMP, Optimization.jl or JuliaSmoothOptimizers interfaces."
  },
  {
    "objectID": "optimization.html#optimisers",
    "href": "optimization.html#optimisers",
    "title": "Optimization",
    "section": "Optimisers",
    "text": "Optimisers\nOptimisers.jl has a collection of gradient descent variations designed for use in machine learning. A reasonable choice for problems with a very large number of variables."
  },
  {
    "objectID": "optimization.html#covariance-matrix-adaption-evolution-strategy",
    "href": "optimization.html#covariance-matrix-adaption-evolution-strategy",
    "title": "Optimization",
    "section": "Covariance Matrix Adaption Evolution Strategy",
    "text": "Covariance Matrix Adaption Evolution Strategy\n\nCMAES can be effective for problems with many local minimum while retaining some of the benefits of Newton’s method. It can even deal with noisy and discontinuous objective functions. CMAEvolutionStrategy is a Julia implementation. GCMAES implements both the original CMAES algorithm and a variation that also uses gradient information.\n\nimport CMAEvolutionStrategy, GCMAES\nβ = ones(10)\nf =  cueiv(β; γ = I + zeros(length(β),length(β)))[1]\nβ0 = zero(β)\n\nout = CMAEvolutionStrategy.minimize(f, β0, 1.0;\n                                    lower = nothing,\n                                    upper = nothing,\n                                    popsize = 4 + floor(Int, 3*log(length(β0))),\n                                    verbosity = 1,\n                                    seed = rand(UInt),\n                                    maxtime = nothing,\n                                    maxiter = 500,\n                                    maxfevals = nothing,\n                                    ftarget = nothing,\n                                    xtol = nothing,\n                                    ftol = 1e-11)\n@show CMAEvolutionStrategy.xbest(out)\n\nxmin, fmin, status = GCMAES.minimize(f, β0, 1.0, fill(-10.,length(β0)), fill(10., length(β0)), maxiter=500)\n@show xmin\n\nimport ForwardDiff\n∇f(β) = ForwardDiff.gradient(f, β)\nxmin, fmin, status=GCMAES.minimize((f, ∇f), β0, 1.0, fill(-10.,length(β0)), fill(10., length(β0)), maxiter=500)\n@show xmin"
  },
  {
    "objectID": "approximation.html#function-approximation",
    "href": "approximation.html#function-approximation",
    "title": "Function Approximation",
    "section": "Function Approximation",
    "text": "Function Approximation\n\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\def\\argmin{\\mathrm{arg}\\min}\n\\]\n\nTarget: \\(f_0:\\R^d \\to \\R\\)\n\nExpensive to compute\n\nGiven \\(\\{x_i, f_0(x_i) \\}_{i=1}^n\\), approximate \\(f_0(x)\\) by \\(\\tilde{f}(x)\\) for \\(\\tilde{f} \\in \\mathcal{F}_k\\)\n\nClass of approximating function \\(\\mathcal{F}_k\\)\nWant \\(\\Vert f_0 - \\tilde{f} \\Vert\\) to be small"
  },
  {
    "objectID": "approximation.html#approximating-classes",
    "href": "approximation.html#approximating-classes",
    "title": "Function Approximation",
    "section": "Approximating Classes",
    "text": "Approximating Classes\n\n\nClassical:\n\nPolynomials\nTrigonometric series\nSplines\nLocal averages / kernel regression\n\nModern:\n\nRadial basis functions\nTrees\nRKHS / Kriging / Gaussian process regression\nNeural networks"
  },
  {
    "objectID": "approximation.html#example-dynamic-programming",
    "href": "approximation.html#example-dynamic-programming",
    "title": "Function Approximation",
    "section": "Example: dynamic programming",
    "text": "Example: dynamic programming\n\nDynamic programming: \\[\nV(x) = \\max_a u(a,x) + \\beta \\Er[V(x') | x, a]\n\\]\n\n\\(x\\) continuous\n\nValue function iteration:\n\nPick grid \\(x_1,..., x_n\\)\nInitial guess \\(\\tilde{V}_0\\)\nMaximize \\(V_1(x_i) = \\max_a u(a,x) + \\beta \\Er[\\tilde{V}_0(x') | x , a]\\)\nSet \\(\\tilde{V}_1 \\approx V_1\\), repeat"
  },
  {
    "objectID": "approximation.html#example-non-parametric-regression",
    "href": "approximation.html#example-non-parametric-regression",
    "title": "Function Approximation",
    "section": "Example: non parametric regression",
    "text": "Example: non parametric regression\n\nObserve \\[\ny_i = f_0(x_i) + \\epsilon_i\n\\]\nEstimate \\[\n\\hat{f}_n \\in \\argmin_{f\\in \\mathcal{F}_k} \\frac{1}{n} \\sum_i (y_i - f(x_i))^2\n\\]\nTypically, \\[\n\\Vert f_0 - \\hat{f}_n \\Vert^2 = \\underbrace{\\Vert f_0 - \\tilde{f} \\Vert^2}_{\\text{approximation error}} + \\underbrace{\\Vert \\tilde{f} - \\hat{f}_n \\Vert^2}_{\\text{variance}}\n\\]\n\n\nI say “typically” because one has to be careful about what these norms are to get a results along these lines."
  },
  {
    "objectID": "approximation.html#vert-f_0---f-vert",
    "href": "approximation.html#vert-f_0---f-vert",
    "title": "Function Approximation",
    "section": "\\(\\Vert f_0 - f \\Vert\\)",
    "text": "\\(\\Vert f_0 - f \\Vert\\)\n\n\nGuarantees of the form \\[\n\\sup_{f_0 \\in \\mathcal{F}_0\n} \\min_{\\hat{f} \\in \\mathcal{F}_k} \\Vert f_0 - \\hat{f} \\Vert_{\\mathcal{V}} \\leq g(n,k)\n\\]\n\n\\(\\mathcal{V}\\) some vector space of functions, \\(\\mathcal{F}_0 \\subseteq \\mathcal{V}\\), \\(\\mathcal{F}_k \\subseteq \\mathcal{V}\\)\nUsually \\(\\lim_{n,k \\to \\infty} g(n,k) = 0\\)\n\nQuality of approximation depends on\n\nrestrictions on \\(\\mathcal{F}_0\\)\n\\(\\mathcal{F}_k\\) that is a good match for \\(\\mathcal{F}_0\\)\n\n\\(\\Vert \\cdot \\Vert_{\\mathcal{V}}\\) might not be norm of interest in applications, so might also want e.g. \\(\\Vert \\cdot \\Vert_{L^q(P(x))} \\leq C \\Vert \\cdot \\Vert_{\\mathcal{V}}\\)"
  },
  {
    "objectID": "approximation.html#polynomials-as-universal-approximators",
    "href": "approximation.html#polynomials-as-universal-approximators",
    "title": "Function Approximation",
    "section": "Polynomials as Universal Approximators",
    "text": "Polynomials as Universal Approximators\n\nStone–Weierstrass theorem if \\(f_0 \\in C(X)\\) for a compact set \\(X \\subset \\R\\), then \\(\\forall \\epsilon&gt;0\\), \\(\\exists\\) polynomial \\(p\\) s.t. \\[\n\\Vert f_0 - p \\Vert_\\infty &lt; \\epsilon\n\\]"
  },
  {
    "objectID": "approximation.html#polynomials",
    "href": "approximation.html#polynomials",
    "title": "Function Approximation",
    "section": "Polynomials",
    "text": "Polynomials\n\nLet \\(P_n\\) be polynomials of order \\(n\\)\nJackson’s theorem (Jackson (1912) and later refinements) if \\(f \\in C^s[-1,1]\\) \\[\n\\inf_{p \\in P_n} \\Vert f - p \\Vert_{\\infty} \\leq \\underbrace{\\left( \\frac{\\pi}{2} \\right)^s \\prod_{j=n - s + 1}^{n+1} \\frac{1}{j}}_{C(s) n^{-s}} \\Vert f^{(s)} \\Vert_\\infty\n\\]\n\nAn existence result, but does not tell how to find optimal \\(p\\)\nIn interpolation, achieving minimum requires careful choice of \\(\\{x_i\\}_{i=1}^n\\)"
  },
  {
    "objectID": "approximation.html#polynomials-1",
    "href": "approximation.html#polynomials-1",
    "title": "Function Approximation",
    "section": "Polynomials",
    "text": "Polynomials\n\nLagrange interpolation problem\nGiven \\(\\{x_i, f(x_i)\\}_{i=1}^n\\), find \\(p_{n-1} \\in P_{n-1}\\) such that \\(p_{n-1}(x_i) = f(x_i)\\)\nIf \\(f \\in C^n[-1,1]\\) \\[\n|f(x) - p_{n-1}(x) | \\leq \\frac{\\Vert f^{(n)} \\Vert_\\infty}{n!}\\prod_{i=1}^n(x - x_i)\n\\]\n\n\n\nChoose \\(x_i\\) to minimize \\(\\Vert \\prod_{i=1}^n(x - x_i) \\Vert\\)\n\nChebyshev polynomials interpolation"
  },
  {
    "objectID": "approximation.html#other-series-estimators",
    "href": "approximation.html#other-series-estimators",
    "title": "Function Approximation",
    "section": "Other Series Estimators",
    "text": "Other Series Estimators\n\nBelloni et al. (2015) and reference therein\nFor smooth series, \\(f_0:\\R^d \\to \\R\\), \\(s\\)-times differentiable, \\[\n\\min_{\\tilde{f} \\in \\mathcal{F}_k} \\Vert f_0 - \\tilde{f} \\Vert \\leq C k^{-s/d}\n\\]\nSplines of order \\(s_0\\), \\[\n\\min_{\\tilde{f} \\in \\mathcal{F}_k} \\Vert f_0 - \\tilde{f} \\Vert \\leq C k^{-\\min\\{s,s_0\\}/d}\n\\]\n\n\nThe constant \\(C\\) is not really constant, but \\(k^{-s/d}\\) is the dominant term"
  },
  {
    "objectID": "approximation.html#reproducing-kernel-hilbert-space",
    "href": "approximation.html#reproducing-kernel-hilbert-space",
    "title": "Function Approximation",
    "section": "Reproducing Kernel Hilbert Space",
    "text": "Reproducing Kernel Hilbert Space\n\nIske et al. (2018) chapter 8\nEspecially useful in multi-dimensional settings\nMairhuber-Curtis theorem implies \\(\\mathcal{F}_k\\) should depend on \\(\\{x_i\\}_{i=1}^n\\) when \\(d \\geq 2\\)"
  },
  {
    "objectID": "approximation.html#from-kernel-to-hilbert-space-moore-aronszajn-theorem",
    "href": "approximation.html#from-kernel-to-hilbert-space-moore-aronszajn-theorem",
    "title": "Function Approximation",
    "section": "From Kernel to Hilbert Space (Moore-Aronszajn theorem)",
    "text": "From Kernel to Hilbert Space (Moore-Aronszajn theorem)\n\nKernel \\(k(\\cdot,\\cdot): \\R^d \\times \\R^d \\to \\R\\) continuous, symmetric, and positive definite \\[\n\\sum_{i=1}^n \\sum_{j=1}^n c_i k(x_i,x_j) c_j \\geq 0 \\forall c, x \\in \\R^n\n\\]\nConstruct an associated Hilbert space \\[\n\\mathcal{K}^{pre} = \\mathrm{span}\\{ \\sum_{j=1}^m c_j k(x_j, \\cdot): c_j \\in \\R, x_j \\in \\R^d, m \\in \\mathbb{N}\\}\n\\] with inner product defined by \\[\n\\langle k(x,\\cdot), k(y,\\cdot) \\rangle_{\\mathcal{K}} = k(x,y)\n\\] completion of \\(\\mathcal{K}^{pre}\\) is a Hilbert space"
  },
  {
    "objectID": "approximation.html#rkhs-facts",
    "href": "approximation.html#rkhs-facts",
    "title": "Function Approximation",
    "section": "RKHS Facts",
    "text": "RKHS Facts\n\n\\(\\langle k(x,\\cdot), f \\rangle_{\\mathcal{K}} = f\\)\n\\(\\mathcal{K} \\subset C(\\R^d)\\)"
  },
  {
    "objectID": "approximation.html#from-hilbert-space-to-kernel",
    "href": "approximation.html#from-hilbert-space-to-kernel",
    "title": "Function Approximation",
    "section": "From Hilbert Space to Kernel",
    "text": "From Hilbert Space to Kernel\n\nIf \\(\\mathcal{H}\\) is a Hilbert space and \\(f \\to f(x)\\) is bounded for all \\(x\\), then \\(\\mathcal{H}\\) is an RKHS"
  },
  {
    "objectID": "approximation.html#interpolation-in-rkhs",
    "href": "approximation.html#interpolation-in-rkhs",
    "title": "Function Approximation",
    "section": "Interpolation in RKHS",
    "text": "Interpolation in RKHS\n\nGiven \\(\\{x_i, f(x_i)\\}_{i=1}^n\\), there is unique \\(\\tilde{f} \\in S_X \\equiv \\{ \\sum_{i=1}^n c_i k(x_i, \\cdot) \\}\\) such that \\(\\tilde{f}(x_i) = f(x_i)\\) -\\(\\tilde{f}\\) solves \\[\n\\min_{g \\in \\mathcal{K}} \\Vert g \\Vert_{\\mathcal{K}} \\, s.t. \\, g(x_i) = f(x_i) \\text{ for } i=1,...,n\n\\]"
  },
  {
    "objectID": "approximation.html#mercers-theorem-and-feature-maps",
    "href": "approximation.html#mercers-theorem-and-feature-maps",
    "title": "Function Approximation",
    "section": "Mercer’s theorem and feature maps",
    "text": "Mercer’s theorem and feature maps\n\n\nGiven another Hilbert space \\(\\mathcal{W}\\), \\(\\Phi: \\R^d \\to \\mathcal{W}\\) is a feature map for \\(k\\) if \\[\nk(x,y) = \\langle \\Phi(x), \\Phi(y) \\rangle_\\mathcal{W}\n\\]\n\nE.g. \\(\\mathcal{W} = \\mathcal{K}\\) and \\(\\Phi(x) = k(x,\\cdot)\\)\n\nGiven measure \\(\\mu\\), define \\(T: L^2(\\mu) \\to L^2(\\mu)\\) by \\[\nT(f)(x) = \\int k(x,y) f(y) d\\mu(y)\n\\]"
  },
  {
    "objectID": "approximation.html#rkhs-as-universal-approximator",
    "href": "approximation.html#rkhs-as-universal-approximator",
    "title": "Function Approximation",
    "section": "RKHS as Universal Approximator",
    "text": "RKHS as Universal Approximator\n\n\nMicchelli, Xu, and Zhang (2006) or summary by Zhang (2018)\nGiven \\(f \\in C(Z)\\), \\(Z \\subset \\R^d\\) and compact, can elements of \\(\\mathcal{K}\\) approximate \\(f\\) in \\(\\Vert\\cdot\\Vert_\\infty\\)?\nLet \\(K(Z) = \\overline{\\mathrm{span}}\\{k(\\cdot,y): y \\in Z\\} \\subseteq C(Z)\\)\nLet \\(\\Phi\\) be feature map for \\(k\\) associated with \\(\\mu\\) with \\(supp(\\mu) = Z\\)\nDual space of \\(C(Z)\\) is set of Borel measures on \\(Z\\), \\(B(Z)\\) with \\[\n\\mu(f) = \\int_Z f d\\mu\n\\]"
  },
  {
    "objectID": "approximation.html#rkhs-as-universal-approximator-1",
    "href": "approximation.html#rkhs-as-universal-approximator-1",
    "title": "Function Approximation",
    "section": "RKHS as Universal Approximator",
    "text": "RKHS as Universal Approximator\n\n\nMap \\(B(Z)\\) into \\(\\mathcal{K}^* = \\mathcal{K}\\) by \\[\n\\langle U(\\mu), h \\rangle_{\\mathcal{K}} = \\langle \\int_Z \\Phi(x)(\\cdot) d\\mu(x), h \\rangle_{\\mathcal{K}} = \\int_Z \\langle \\Phi(x), h \\rangle_{\\mathcal{K}} d\\mu(x)\n\\]\n\\(U\\) is bounded\n\\(K(Z)^\\perp = \\mathcal{N}(U)\\), i.e. universal approximation iff \\(\\mathcal{N}(U) = \\{0\\}\\).\n\niff \\(\\overline{\\mathrm{span}}\\{ \\langle \\Phi(x), \\gamma_i \\rangle_\\mathcal{K} : \\gamma_i \\text{ basis for } \\mathcal{K} \\} = C(Z)\\)\n\nRadial kernels are universal approximators \\[\nk(x,y) = \\int e^{-t\\Vert x-y\\Vert^2} d\\nu(t)\n\\]"
  },
  {
    "objectID": "approximation.html#rkhs-approximation-rate",
    "href": "approximation.html#rkhs-approximation-rate",
    "title": "Function Approximation",
    "section": "RKHS approximation rate",
    "text": "RKHS approximation rate\n\nBach (2024) chapter 7\nIf \\(k\\) is translation invariant and \\(f\\) is \\(s\\) times differentiable, \\[\n\\inf_{f \\in \\mathcal{K}} \\Vert f - f_0 \\Vert_{L^2(\\mu)} + \\lambda \\Vert f \\Vert_{\\mathcal{K}} \\approx O(\\lambda^(s/r_k))\n\\] where \\(r_k\\) depends on \\(k\\) and need \\(r_k &gt; d/2 &gt; s\\) (if \\(s \\geq r_k\\), then \\(f_0 \\in \\mathcal{K}\\) and can do better)\nIf sample \\(x_i \\sim \\mu\\) and interpolate \\[\n\\hat{f} = \\argmin_{f \\in \\mathcal{K}} \\frac{1}{n}\\sum_{i=1}^n (f_0(x_i) - f(x_i) )^2 + \\lambda \\Vert f \\Vert_{\\mathcal{K}}\n\\] then \\(\\Vert \\hat{f} - f_0 \\Vert_{L^2(\\mu)} \\approx O(n^{-s/r_k})\\)\n\nif \\(f_0 \\in \\mathcal{K}\\), \\(O(n^{-1/2})\\)"
  },
  {
    "objectID": "approximation.html#neural-networks",
    "href": "approximation.html#neural-networks",
    "title": "Function Approximation",
    "section": "Neural Networks",
    "text": "Neural Networks\n\nBach (2024)"
  },
  {
    "objectID": "approximation.html#investment-with-adjustment-costs-and-productivity",
    "href": "approximation.html#investment-with-adjustment-costs-and-productivity",
    "title": "Function Approximation",
    "section": "Investment with adjustment costs and productivity",
    "text": "Investment with adjustment costs and productivity\n\nPrice taking firm, output \\(y = e^\\omega F(k,\\ell,m)\\), prices \\(p_y, p_m, p_\\ell, p_k\\)\nEach period, \\(m\\), flexibly chosen given predetermined \\(k, \\ell\\) \\[\n\\max_{m} p_y e^\\omega F(k,\\ell,m) - p_m m\n\\]\n\n\nusing ModelingToolkit, NonlinearSolve, Symbolics\n@variables m\n@parameters k, l, ω, pm, py\nDm = Differential(m)\nproduction(k,l,m) = k^(2//10)*l^(4//10)*m^(3//10)\n#ρ = 5\n#rts = 9//10\n#ces(k,l,m) = (k^ρ + l^ρ + m^ρ)^(rts/ρ)\nprofits(k,l,m,ω,pm,py) = py*exp(ω)*production(k,l,m) - pm*m\nmstar = symbolic_solve(Symbolics.derivative(profits(k,l,m,ω,pm,py),m) ~ 0 ,m)[1]\nprofits(k,l,mstar,ω,pm,py)\n\nflowprofits = eval(build_function(profits(k,l,mstar,ω,pm,py), k,l,ω,pm,py))\n\n#1 (generic function with 1 method)"
  },
  {
    "objectID": "approximation.html#dynamic-labor-and-capital-choices",
    "href": "approximation.html#dynamic-labor-and-capital-choices",
    "title": "Function Approximation",
    "section": "Dynamic Labor and Capital Choices",
    "text": "Dynamic Labor and Capital Choices\n\n\\(\\omega_t\\) Markov, \\(F(\\omega_t | \\omega_{t-1})\\), prices constant\nLabor and capital chosen before \\(\\omega_t\\) known\nAdjustment cost of capital \\(c(k',k)\\)\n\n\\[\n\\begin{align*}\n  V(\\omega, k,\\ell) = & \\max_{k',\\ell',m} p_y e^\\omega F(k,\\ell,m) - p_\\ell \\ell - p_k k - c(k,k') + \\beta E_\\omega' [V(\\omega', k',\\ell') | \\omega ] \\\\\n  = & \\max_{k',\\ell'} \\pi^*(p,\\omega, k, \\ell) - c(k,k') + \\beta E[V(\\omega',k',\\ell')|\\omega]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "approximation.html#section",
    "href": "approximation.html#section",
    "title": "Function Approximation",
    "section": "",
    "text": "using Surrogates"
  },
  {
    "objectID": "approximation.html#mercers-theorem-and-feature-maps-1",
    "href": "approximation.html#mercers-theorem-and-feature-maps-1",
    "title": "Function Approximation",
    "section": "Mercer’s theorem and feature maps",
    "text": "Mercer’s theorem and feature maps\n\n\nMercer’s theorem gives existence of Eigen decomposition, \\[\nT(f)(x) = \\sum_{i=1}^\\infty \\lambda_i \\phi_i(x) \\langle \\bar{\\phi_i}, f \\rangle_{L^2(\\mu)}\n\\] and \\[\nk(x,y) = \\sum \\lambda_i \\phi_i(x) \\bar{\\phi_i}(y)\n\\]\nFeature map \\(k(x,y) = \\langle (\\sqrt{\\lambda_i} \\phi_i(x))_{i=1}^\\infty, (\\sqrt{\\lambda_i} \\phi_i(x))_{i=1}^\\infty \\rangle_{\\ell^2}\\)\nLink between inner product and norm in \\(\\mathcal{K}\\) and in \\(L^2(\\mu)\\)"
  },
  {
    "objectID": "approximation.html#references",
    "href": "approximation.html#references",
    "title": "Function Approximation",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBach, Francis. 2024. Learning Theory from First Principles. MIT Press. https://www.di.ens.fr/~fbach/ltfp_book.pdf.\n\n\nBelloni, Alexandre, Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. 2015. “Some New Asymptotic Theory for Least Squares Series: Pointwise and Uniform Results.” Journal of Econometrics 186 (2): 345–66. https://doi.org/https://doi.org/10.1016/j.jeconom.2015.02.014.\n\n\nIske, Armin et al. 2018. Approximation Theory and Algorithms for Data Analysis. Springer. https://link.springer.com/book/10.1007/978-3-030-05228-7.\n\n\nJackson, Dunham. 1912. “On Approximation by Trigonometric Sums and Polynomials.” Transactions of the American Mathematical Society 13 (4): 491–515. http://www.jstor.org/stable/1988583.\n\n\nMicchelli, Charles A., Yuesheng Xu, and Haizhang Zhang. 2006. “Universal Kernels.” Journal of Machine Learning Research 7 (95): 2651–67. http://jmlr.org/papers/v7/micchelli06a.html.\n\n\nZhang, Thomas. 2018. “Reproducing Kernel Hilbert Spaces.” https://thomaszh3.github.io/writeups/RKHS.pdf."
  }
]