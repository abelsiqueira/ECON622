---
title: Solving Nonlinear Equations
author: Paul Schrimpf
date: last-modified
bibliography: 622.bib
format:
  revealjs:
    theme: blood
    smaller: true
    min-scale: 0.1
    max-scale: 3.0
    chalkboard:
      theme: whiteboard
      boardmarker-width: 2
      chalk-width: 2
      chalk-effect: 0.0
engine: julia
---

$$
\def\Er{{\mathrm{E}}}
\def\En{{\mathbb{E}_n}}
\def\cov{{\mathrm{Cov}}}
\def\var{{\mathrm{Var}}}
\def\R{{\mathbb{R}}}
\def\arg{{\mathrm{arg}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\def\rank{{\mathrm{rank}}}
\newcommand{\inpr}{ \overset{p^*_{\scriptscriptstyle n}}{\longrightarrow}}
\def\inprob{{\,{\buildrel p \over \rightarrow}\,}}
\def\indist{\,{\buildrel d \over \rightarrow}\,}
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
$$

# Nonlinear Equations

## Nonlinear Equations

- $F: \R^n \to \R^n$
- Want to solve for $x$
$$
F(x) = 0
$$


## Example: BLP

- Share equation
$$
s_{j} = \int \frac{e^{\delta_{j} + x_{j}'\Sigma \nu}} {\sum_{k = 0}^J e^{\delta_{k} + x_{k}'\Sigma \nu}} dF\nu
$$
- $J$ equations to solve for $\delta = (\delta_{1t}, ..., \delta_{Jt})$

## Newton's Method

- $F(x)$ differentiable with Jacobian $F'(x)$
- Algorithm:
  1. Initial guess $x_0$
  2. Update based on first order expansion
  $$
  \begin{align*}
  F(x_{s+1}) \approx F(x_s) + F'(x_s)(x_{s+1} - x_s) = & 0
  x_{s+1} = x_s + F'(x_s) \ F(x_s)
  \end{align*}
  $$
  3. Repeat until $\Vert F(x_s) \Vert \approx 0$

## Simple Idea, Many Variations

- Step size
  1. Initial guess $x_0$
  2. Update based on first order expansion
  $$
  \begin{align*}
  F(x_{s+1}) \approx F(x_s) + F'(x_s)(x_{s+1} - x_s) = & 0
  x_{s+1} = x_s + {\color{red}{\lambda}} F'(x_s) \ F(x_s)
  \end{align*}
  $$
  3. Repeat until $\Vert F(x_s) \Vert \approx 0$

- line search or trust region

## Simple Idea, Many Variations

-
  1. Initial guess $x_0$
  2. Update based on first order expansion
  $$
  \begin{align*}
  F(x_{s+1}) \approx F(x_s) + F'(x_s)(x_{s+1} - x_s) = & 0
  \end{align*}
  $$
  approximately solve
  $$
  F'(x_s) A = F(x_s)
  $$
  update
  $$
  x_{s+1} = x_s + \lambda {\color{red}{A}}
  $$
  3. Repeat until $\Vert F(x_s) \Vert \approx 0$

- Especially if $F'(x_s)$ is large and/or sparse

## Simple Idea, Many Variations

-
  1. Initial guess $x_0$
  2. Update based on first order expansion
  $$
  \begin{align*}
  F(x_{s+1}) \approx F(x_s) + F'(x_s)(x_{s+1} - x_s) = & 0
  x_{s+1} = x_s + F'(x_s) \ F(x_s)
  \end{align*}
  $$
  3. Repeat until ${\color{red}{\Vert F(x_{s+1}) \Vert < rtol \Vert
      F(x_0) \Vert + atol }}$

## Simple Idea, Many Variations

-
  1. Initial guess $x_0$
  2. Update based on first order expansion
     - compute $F'(x_s)$ using:
       - hand written code or
       - finite differences or
       - secant method $F'(x_s) \approx \frac{F(x_s) - F(x_{s-1})}{\Vert x_s - x_{s-1} \Vert}$ or
       - automatic differentiation
  $$
  \begin{align*}
  F(x_{s+1}) \approx F(x_s) + F'(x_s)(x_{s+1} - x_s) = & 0
  x_{s+1} = x_s + F'(x_s) \ F(x_s)
  \end{align*}
  $$
  3. Repeat until $\Vert F(x_{s+1}) \Vert \approx 0$

## Simple Idea, Many Variations

- @kelley2022 is thorough reference for nonlinear equation solving methods and their properties
- [NonlinearSolve.jl](https://docs.sciml.ai/NonlinearSolve/stable/) gives unified interface for many methods

# Examples

## BLP share equation

```{julia}
@doc raw"""
    share(δ, Σ, dFν, x)

Computes shares in random coefficient logit with mean tastes `δ`, observed characteristics `x`, unobserved taste distribution `dFν`, and taste covariances `Σ`.

# Arguments

- `δ` vector of length `J`
- `Σ` `K` by `K` matrix
- `dFν` distribution of length `K` vector
- `x` `J` by `K` array

# Returns

- vector of length `J` consisting of $s_1$, ..., $s_J$
"""
function share(δ, Σ, dFν, x, ∫ = ∫cuba)
  J,K = size(x)
  (length(δ) == J) || error("length(δ)=$(length(δ)) != size(x,1)=$J")
  (K,K) === size(Σ) || error("size(x,2)=$K != size(Σ)=$(size(Σ))")
  function shareν(ν)
    s = δ + x*Σ*ν
    s .-= maximum(s)
    s .= exp.(s)
    s ./= sum(s)
    return(s)
  end
  return(∫(shareν, dFν))
end

using HCubature
function ∫cuba(f, dx; rtol=1e-4)
  D = length(dx)
  x(t) = t./(1 .- t.^2)
  Dx(t) = prod((1 .+ t.^2)./(1 .- t.^2).^2)
  hcubature(t->f(x(t))*pdf(dx,x(t))*Dx(t), -ones(D),ones(D), rtol=rtol)[1]
end

using SparseGrids, FastGaussQuadrature, Distributions
function ∫sgq(f, dx::MvNormal; order=5)
  X, W = sparsegrid(length(dx), order, gausshermite, sym=true)
  L = cholesky(dx.Σ).L
  sum(f(√2*L*x + dx.μ)*w for (x,w) ∈ zip(X,W))/(π^(length(dx)/2))
end
```

## Solving for $\delta$

```{julia}
# create a problem to solve
using LinearAlgebra
J = 3
K = 2
x = randn(J,K)
C = randn(K,K)
Σ = C'*C + I
δ = randn(J)
dFν = MvNormal(zeros(K), I)
s = share(δ, Σ, dFν, x, ∫sgq)

# try to recover δ
using NonlinearSolve, LinearAlgebra
F(δ, Σ) = share(δ,Σ,dFν, x,∫sgq) - s
δ0 = zero(δ)
prob = NonlinearProblem(F, δ0, Σ)
sol = solve(prob)


println("True δ: $δ")
println("Solved δ: $(sol.u)")
println("||F(sol.u)||: $(norm(sol.resid))")
println("Error: $(norm(sol.u - δ))")
```

## Alternative algorithms

```{julia}
solr = solve(prob, RobustMultiNewton(), show_trace=Val(true), trace_level=TraceAll())
```

```{julia}
using FixedPointAcceleration
solfp = solve(prob, FixedPointAccelerationJL() , show_trace=Val(true), trace_level=TraceAll())
```


# References
