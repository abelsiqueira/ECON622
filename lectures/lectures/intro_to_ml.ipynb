{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECON622: Computational Economics with Data Science Applications\n",
        "\n",
        "Key Terminology and Concepts in Machine Learning\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "# Overview\n",
        "\n",
        "## Motivation\n",
        "\n",
        "-   In this lecture we will introduce some key terminology and concepts\n",
        "    in machine learning\n",
        "-   We will map these concepts to terminology in economics and\n",
        "    statistics\n",
        "-   Finally, we will discuss Python Frameworks\n",
        "\n",
        "## Textbooks\n",
        "\n",
        "-   [Probabilistic Machine Learning: An\n",
        "    Introduction](https://probml.github.io/pml-book/book1.html) by Kevin\n",
        "    Murphy\n",
        "-   [Probabilistic Machine Learning: Advanced\n",
        "    Topics](https://probml.github.io/pml-book/book2.html) by Kevin\n",
        "    Murphy\n",
        "-   Both have PDFs of the “draft” available\n",
        "-   Code is available on https://github.com/probml/pyprobml\n",
        "    -   Much in JAX, some in torch/etc.\n",
        "-   For some linear algebra/etc. examples, see the [ECON526\n",
        "    Lectures](https://ubcecon.github.io/ECON526/lectures/lectures/index.html)\n",
        "\n",
        "## Relevant Categories of Machine Learning\n",
        "\n",
        "-   Supervised Learning (e.g., Regression and Classification)\n",
        "-   Unsupervised Learning (e.g., Clustering, Auto-encoders)\n",
        "-   Semi-Supervised Learning (e.g., some observations)\n",
        "-   Reinforcement Learning (e.g., policy/control)\n",
        "-   Generative Models/Bayesian Methods (e.g., diffusions, probabilistic\n",
        "    programming)\n",
        "\n",
        "Instance-based learning (e.g., Kernel Methods) and Deep Learning are\n",
        "somewhat orthogonal\n",
        "\n",
        "## Key Terminology: Features, Labels, and Latents\n",
        "\n",
        "-   **Features** are economists **explanatory or independent\n",
        "    variables**. They have the key source of variation to make\n",
        "    predictions and conduct counterfactuals\n",
        "-   **Labels** correspond to economists **observables or dependent\n",
        "    variables**\n",
        "-   **Latent Variables** are **unobserved variables**, typically sources\n",
        "    of heterogeneity or which may drive both the dependent and\n",
        "    independent variables\n",
        "-   **Feature Engineering** is the process of creating or selecting\n",
        "    features from the data that are more useful for the task at hand\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "-   20th vs. 21st Century ML\n",
        "-   Stochastic Gradients and Auto-Differentiation\n",
        "-   Implicit and Explicit Regularization\n",
        "-   Inductive/Implicit Bias\n",
        "-   Generalization\n",
        "-   Overfitting and the Bias-Variance Tradeoff\n",
        "-   Test vs. Train vs. Validation Set\n",
        "-   Hyperparameter Optimization\n",
        "-   Representation Learning\n",
        "-   Transfer Learning\n",
        "\n",
        "# Python\n",
        "\n",
        "## Why Python?\n",
        "\n",
        "-   For “modern” ML: **all** the well-supported frameworks are in Python\n",
        "-   In particular, auto-differentiation is central to many ML algorithms\n",
        "-   Why should you avoid Julia/Matlab/R in these cases?\n",
        "    -   Poor AD, especially for reverse-mode\n",
        "    -   Network effects. Very few higher level packages for ML pipeline\n",
        "    -   But Julia dominates for many ML topics (e.g. ODEs) and R is\n",
        "        outstanding for classic ML\n",
        "-   Should you use Python for more things?\n",
        "    -   Maybe, but it is limited and can be slow unless you jump through\n",
        "        hoops\n",
        "    -   Personally, if I have algorithms but no need for AD or\n",
        "        particular packages, Julia is a much better language and less\n",
        "        frustrating\n",
        "\n",
        "## There is no Such Thing as “Python”!\n",
        "\n",
        "-   Many incompatible wrappers around C++ for numerical methods\n",
        "-   Numpy/Scipy is the baseline (a common API)\n",
        "-   Pytorch\n",
        "-   JAX\n",
        "-   Ones to avoid\n",
        "    -   Tensorflow, common in industry but old\n",
        "    -   Numba (for me, reasonable people disagree)\n",
        "\n",
        "## Pytorch\n",
        "\n",
        "-   In recent years, the most flexible and popular ML framework for\n",
        "    researchers\n",
        "-   Key features:\n",
        "    -   Most of the code is for auto-differentiation/GPUs\n",
        "    -   JIT/etc. for GPU and fast kernels for deep learning\n",
        "    -   Neural Network libraries and utilities\n",
        "    -   A good subset of numpy\n",
        "    -   Utilities for ML pipelines optimization/etc.\n",
        "\n",
        "## Pytorch Key Downsides\n",
        "\n",
        "-   Not really for general purpose programming\n",
        "    -   Intended for making auto-differentiation of neural networks\n",
        "        easy, and updating gradients for solvers\n",
        "    -   May be very slow for simple things or ones which don’t involve\n",
        "        high-dimensional AD\n",
        "-   Won’t always have packages you need for general code, and\n",
        "    compatibility is ugly\n",
        "\n",
        "## JAX\n",
        "\n",
        "-   Compiler that enables layered program transformations\n",
        "    1.  `jit` compiler to [XLA](https://www.tensorflow.org/xla/),\n",
        "        including accelerators (e.g. GPUs)\n",
        "    2.  `grad` Auto-differentiation\n",
        "    3.  `vmap` vectorization\n",
        "    4.  Flexibility to add more transformations\n",
        "-   [JAX\n",
        "    PyTrees](https://jax.readthedocs.io/en/latest/jax-101/05.1-pytrees.html)\n",
        "    provide a nested tree structure for compiler passes\n",
        "-   Closer to being a full JIT for general code than pytorch\n",
        "-   For ML, not full-featured like pytorch. Need to shop for other\n",
        "    libraries\n",
        "\n",
        "## JAX Key Downsides\n",
        "\n",
        "-   Tough to trust Google, especially since it is a research project\n",
        "    -   Too ingrained in DeepMind research to disappear, but might have\n",
        "        intermittent support\n",
        "-   Different operating system support is limited, but they are making\n",
        "    progress\n",
        "-   Subset of python. Can’t really use loops, etc. Functional-style\n",
        "    programming\n",
        "    -   Much more restrictive than it seems, and far more restrictive\n",
        "        than pytorch\n",
        "\n",
        "# Python Ecosystem\n",
        "\n",
        "## Environments\n",
        "\n",
        "-   Hate it, but nevertheless [install\n",
        "    conda](https://www.anaconda.com/download)\n",
        "-   **Always** use a virtual environment to hate conda slightly less\n",
        "-   Keep dependencies in `requirements.txt` for `pip`, conda\n",
        "    `environment.yml` if necessary\n",
        "-   To create, activate, and install packages\n",
        "\n",
        "```` markdown\n",
        "```{bash}\n",
        "conda create -n econ622 python=3.11\n",
        "conda activate econ622\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "````\n",
        "\n",
        "-   In vscode, you can go `> Python: Select Interpreter`, choose\n",
        "    `econ622` and it will automatically activate\n",
        "\n",
        "## Development Environment\n",
        "\n",
        "-   pip/anaconda with virtual environments\n",
        "-   Use VSCode for debugging, testing, etc.\n",
        "-   Github Copilot. Install [Github Copilot\n",
        "    Chat](https://docs.github.com/en/copilot/github-copilot-chat/using-github-copilot-chat)\n",
        "-   Format with `black`. See\n",
        "    [here](https://code.visualstudio.com/docs/python/formatting) for\n",
        "    setup\n",
        "\n",
        "## Baseline, Safe Packages to Use\n",
        "\n",
        "-   [Numpy](https://numpy.org/doc/stable/) and\n",
        "    [Scipy](https://docs.scipy.org/doc/scipy/reference/)\n",
        "-   [Pandas](https://pandas.pydata.org/docs/) for dataframes\n",
        "-   [Matplotlib](https://matplotlib.org/stable/contents.html) for\n",
        "    general plotting\n",
        "-   [Seaborn](https://seaborn.pydata.org/) for plotting data\n",
        "-   [Statsmodels](https://www.statsmodels.org/stable/index.html) for\n",
        "    classic econometrics\n",
        "-   [Scikit-learn](https://scikit-learn.org/stable/) for classic ML\n",
        "\n",
        "## Numpy/Scipy Basics\n",
        "\n",
        "-   Using some examples from\n",
        "    [ECON526](https://ubcecon.github.io/ECON526/lectures/lectures/index.html)\n",
        "\n",
        "## General Tools for ML Pipelines\n",
        "\n",
        "-   Logging/visualization: [Weights and Biases](https://wandb.ai/site)\n",
        "    -   Sign up for an account! Built in hyperparameter optimization\n",
        "        tools\n",
        "-   CLI useful for many pipelines and HPO. See\n",
        "    [here](https://github.com/shadawck/awesome-cli-frameworks#python)\n",
        "-   For more end-to-end frameworks for deep-learning\n",
        "    -   [Pytorch Lightning](https://www.pytorchlightning.ai/) is\n",
        "        extremely easy and flexble, eliminating a lot of boilerplate for\n",
        "        CLI, optimizers, GPUs, etc.\n",
        "    -   [Keras](https://keras.io/) is a higher-level framework for deep\n",
        "        learning. Traditionally tensorflow, but now many. Also\n",
        "        [FastAI](https://www.fast.ai/)\n",
        "-   [HuggingFace](https://huggingface.co/) is a great resource for NLP\n",
        "    and transformers\n",
        "-   [Optuna](https://optuna.org/) is a great hyperparameter optimization\n",
        "    framework, etc.\n",
        "\n",
        "# JAX Ecosystem\n",
        "\n",
        "## Examples of Core Transformations\n",
        "\n",
        "From [JAX\n",
        "quickstart](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)\n",
        "\n",
        "Builtin composable transformations: `jit`, `grad` and `vmap`"
      ],
      "id": "38e7625f-808d-472a-9eac-0b8ab72e75ca"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from jax import grad, jit, vmap, random"
      ],
      "id": "abd21621"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compiling with `jit`"
      ],
      "id": "cd9c8c01-47bd-471f-a536-26063d93d455"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.64 ms ± 16 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
            "768 µs ± 2.62 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
          ]
        }
      ],
      "source": [
        "def selu(x, alpha=1.67, lmbda=1.05):\n",
        "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
        "key = random.PRNGKey(0)  \n",
        "x = random.normal(key, (1000000,))\n",
        "%timeit selu(x).block_until_ready()\n",
        "selu_jit = jit(selu)\n",
        "%timeit selu_jit(x).block_until_ready()"
      ],
      "id": "308c057e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convenience Decorators for `jit`\n",
        "\n",
        "-   Convenience python decorator `@jit`"
      ],
      "id": "44038e30-4ddc-4dcf-8411-5b08021dbfba"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "819 µs ± 45.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
          ]
        }
      ],
      "source": [
        "@jit\n",
        "def selu(x, alpha=1.67, lmbda=1.05):\n",
        "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
        "%timeit selu(x).block_until_ready()"
      ],
      "id": "333a692c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Differentiation with `grad`"
      ],
      "id": "131662f4-a8bf-4542-9547-3031f5aae78e"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.25       0.19661197 0.10499357]"
          ]
        }
      ],
      "source": [
        "def sum_logistic(x):\n",
        "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
        "\n",
        "x_small = jnp.arange(3.)\n",
        "derivative_fn = grad(sum_logistic)\n",
        "print(derivative_fn(x_small))"
      ],
      "id": "90abdc7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manual “Batching”/Vectorization\n",
        "\n",
        "Common to run the same function along one dimension of an array"
      ],
      "id": "40f2f8e6-3601-43a3-a2f1-d42b5042b166"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.21 ms ± 55.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
          ]
        }
      ],
      "source": [
        "mat = random.normal(key, (150, 100))\n",
        "batched_x = random.normal(key, (10, 100))\n",
        "\n",
        "def f(v):\n",
        "  return jnp.dot(mat, v)\n",
        "def naively_batched_f(v_batched):\n",
        "  return jnp.stack([f(v) for v in v_batched])\n",
        "%timeit naively_batched_f(batched_x).block_until_ready()  "
      ],
      "id": "cdc3e676"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using `vmap`\n",
        "\n",
        "The\n",
        "[vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)\n",
        "applies across a dimension"
      ],
      "id": "42b41f5c-7ad9-41a3-8260-54dced65f4e2"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-vectorized with vmap\n",
            "23.8 µs ± 562 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
          ]
        }
      ],
      "source": [
        "@jit\n",
        "def vmap_batched_f(v_batched):\n",
        "  return vmap(f)(v_batched)\n",
        "\n",
        "print('Auto-vectorized with vmap')\n",
        "%timeit vmap_batched_f(batched_x).block_until_ready()"
      ],
      "id": "44707267"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic JAX Packages\n",
        "\n",
        "Anything from [Patrick](https://github.com/patrick-kidger) is gold\n",
        "\n",
        "-   Basic program passes/etc:\n",
        "    [JAX](https://jax.readthedocs.io/en/latest/index.html)\n",
        "    -   [jax.numpy](https://jax.readthedocs.io/en/latest/jax.numpy.html#module-jax.numpy)\n",
        "        built in and\n",
        "        [jax.scipy](https://jax.readthedocs.io/en/latest/jax.scipy.html)\n",
        "        which is a subset of scipy\n",
        "-   Neural Network Libraries\n",
        "    -   [Flax](https://flax.readthedocs.io/en/latest/index.html) and\n",
        "        [Haiku](https://dm-haiku.readthedocs.io/en/latest/index.html)\n",
        "    -   [Equinox](https://github.com/patrick-kidger/equinox) is my\n",
        "        favorite\n",
        "-   First-order optimizers for training:\n",
        "    [Optax](https://github.com/google-deepmind/optax)\n",
        "-   Typechecking: [jaxtyping](https://github.com/google/jaxtyping)\n",
        "-   Checkpointing and serialization:\n",
        "    [obax](https://orbax.readthedocs.io/en/latest/)\n",
        "\n",
        "## More Scientific Computing\n",
        "\n",
        "-   Nonlinear Systems/Least Squares:\n",
        "    [Optimistix](https://github.com/patrick-kidger/optimistix)\n",
        "-   Linear Systems of Equations:\n",
        "    [Lineax](https://docs.kidger.site/lineax/)\n",
        "-   Matrix-free operators for iterative solvers:\n",
        "    [COLA](https://github.com/wilson-labs/cola)\n",
        "-   Differential Equations:\n",
        "    [diffrax](https://github.com/patrick-kidger/diffrax)\n",
        "-   More general optimization and solvers:\n",
        "    [JAXopt](https://jaxopt.github.io/stable/#)\n",
        "\n",
        "## JAX Challenges\n",
        "\n",
        "-   Basically only pure functional programming\n",
        "    -   No “mutation” of vectors\n",
        "    -   Loops/conditionals are tough\n",
        "    -   Rules for what is `jit`able are tricky\n",
        "-   See [JAX - The Sharp\n",
        "    Bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html)\n",
        "-   May not be faster on CPUs or for “normal” things\n",
        "-   Debugging\n",
        "\n",
        "## PyTrees\n",
        "\n",
        "-   JAX uses a generic [tree\n",
        "    structure](https://jax.readthedocs.io/en/latest/jax-101/05.1-pytrees.html).\n",
        "    Powerful but takes time to understand: Examples from:\n",
        "    [here](https://jax.readthedocs.io/en/latest/pytrees.html) and\n",
        "    [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)"
      ],
      "id": "0afcc08f-0539-411d-8abd-a5ea59cc96d3"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.0\n",
            "25.0\n",
            "[11. 25.]"
          ]
        }
      ],
      "source": [
        "f = lambda x, y: jnp.vdot(x, y)\n",
        "X = jnp.array([[1.0, 2.0],\n",
        "               [3.0, 4.0]])\n",
        "y = jnp.array([3.0, 4.0])\n",
        "print(f(X[0], y))\n",
        "print(f(X[1], y))\n",
        "\n",
        "mv = vmap(f, in_axes = (\n",
        "  0, # broadcast over 1st index of first argument\n",
        "  None # don't broadcast over anything of second argument\n",
        "  ), out_axes=0)\n",
        "print(mv(X, y))"
      ],
      "id": "286c9dad"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTree Example 1\n",
        "\n",
        "The `in_axes` can match more complicated structures"
      ],
      "id": "d6e9f69e-2da8-4c56-a81b-807e9611a187"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 2. 3. 4. 5.]"
          ]
        }
      ],
      "source": [
        "dct = {'a': 0., 'b': jnp.arange(5.)}\n",
        "def foo(dct, x):\n",
        " return dct['a'] + dct['b'] + x\n",
        "# axes must match shape of the PyTree\n",
        "x = 1.\n",
        "out = vmap(foo, in_axes=(\n",
        "  {'a': None, 'b': 0}, #broadcast over the 'b'\n",
        "  None # no broadcasting over the \"x\"\n",
        "  ))(dct, x)\n",
        "# example now: {'a': 0, 'b': 0} etc.\n",
        "print(out)"
      ],
      "id": "b90bef4b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTree Example 2"
      ],
      "id": "c808a71f-2a84-4bef-8517-86045d494157"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 6. 10.]"
          ]
        }
      ],
      "source": [
        "dct = {'a': jnp.array([3.0, 5.0]), 'b': jnp.array([2.0, 4.0])}\n",
        "def foo2(dct, x):\n",
        " return dct['a'] + dct['b'] + x\n",
        "# axes must match shape of the PyTree\n",
        "x = 1.\n",
        "out = vmap(foo2, in_axes=(\n",
        "  {'a': 0, 'b': 0}, #broadcast over the 'a' and 'b'\n",
        "  None # no broadcasting over the \"x\"\n",
        "  ))(dct, x)\n",
        "# example now: {'a': 3.0, 'b': 2.0} etc.\n",
        "print(out)"
      ],
      "id": "fa30024a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTree Example 3"
      ],
      "id": "8c809b85-3fc8-487e-85f5-27b77cd2d6c8"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16. 17. 18. 19. 20.]"
          ]
        }
      ],
      "source": [
        "dct = {'a': jnp.array([3.0, 5.0]), 'b': jnp.arange(5.)}\n",
        "def foo3(dct, x):\n",
        " return dct['a'][0] * dct['a'][1] + dct['b'] + x\n",
        "# axes must match shape of the PyTree\n",
        "out = vmap(foo3, in_axes=(\n",
        "  {'a': None, 'b': 0}, #broadcast over the 'b'\n",
        "  None # no broadcasting over the \"x\"\n",
        "  ))(dct, x)\n",
        "# example now: {'a': [3.0, 5.0], 'b': 0} etc.\n",
        "print(out)"
      ],
      "id": "0a771583"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  }
}