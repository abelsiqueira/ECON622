{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECON622: Computational Economics with Data Science Applications\n",
        "\n",
        "Optimization for Machine Learning\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "# Overview\n",
        "\n",
        "## Summary\n",
        "\n",
        "-   This lecture continues from the previous lecture on gradients to\n",
        "    further explore optimization methods in machine learning, and\n",
        "    discusses training pipelines and tooling\n",
        "-   Primary reference materials are:\n",
        "    -   [ProbML Book 1:\n",
        "        Introduction](https://probml.github.io/pml-book/book1.html)\n",
        "    -   [ProbML Book 2: Advanced\n",
        "        Topics](https://probml.github.io/pml-book/book2.html) including\n",
        "        Section 6.3\n",
        "    -   [Mark Schmidt’s ML Lecture\n",
        "        Notes](https://www.cs.ubc.ca/~schmidtm/Courses/LecturesOnML/)\n",
        "-   We will also give a sense of a standard machine learning pipeline of\n",
        "    training, validation, and test data and discuss generalization,\n",
        "    logging, etc.\n",
        "\n",
        "## Why the Emphasis on Optimization and Gradients?\n",
        "\n",
        "-   A huge number of algorithms for economists can be written as\n",
        "    optimization problems (e.g., MLE, interpolation) or as something\n",
        "    similar in spirit (e.g. Bayesian Sampling, Reinforcement Learning)\n",
        "-   In practice, **all** problems with high-dimensions parameters or\n",
        "    latents require gradients\n",
        "-   Previous lectures on AD showed we can find gradients for extremely\n",
        "    complicated functions with VJPs\n",
        "\n",
        "# Optimization Crash Course\n",
        "\n",
        "## Optimization Methods\n",
        "\n",
        "-   Learning continuous optimization methods is an enormous project\n",
        "-   See referenced materials and lecture notes\n",
        "-   Here we will give an overview of some key concepts\n",
        "-   Be warned! The details matter, so more study is required if you want\n",
        "    to use these methods in practice\n",
        "\n",
        "## Crash Course in Unconstrained Optimization\n",
        "\n",
        "$$\n",
        "\\min_{\\theta} \\mathcal{L}(\\theta)\n",
        "$$\n",
        "\n",
        "Will briefly introduce\n",
        "\n",
        "-   First-order methods\n",
        "-   Second-order methods\n",
        "-   Preconditioning\n",
        "-   Momentum\n",
        "-   Regularization\n",
        "\n",
        "## First-Order Methods\n",
        "\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.2\n",
        "-   Armed with reverse-mode AD for\n",
        "    $\\mathcal{L} : \\mathbb{R}^N \\to \\mathbb{R}$ we can calculate\n",
        "    $\\nabla \\mathcal{L}(\\theta)$ with the same computational order as\n",
        "    $\\mathcal{L}(\\theta)$\n",
        "-   Furthermore, given JVPs we know we can calculate these objective\n",
        "    functions for extremely complicated functions (e.g., nested fixed\n",
        "    points, and implicit functions)\n",
        "-   Iterative: take $\\theta_0$ and provide $\\theta_t \\to \\theta_{t+1}$\n",
        "    -   May converge to a stationary point (hopefully close to a global\n",
        "        argmin)\n",
        "    -   If it doesn’t converge, the solution may still be an argmin\n",
        "    -   See references for details on convergence for convex and\n",
        "        non-convex problems\n",
        "\n",
        "## Gradient Descent\n",
        "\n",
        "-   See [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L13.pdf)\n",
        "-   Gradient descent takes $\\theta_0$, and stepsize $\\eta_t$ and\n",
        "    iterates until $\\nabla \\mathcal{L}(\\theta_t)$ is small, or\n",
        "    $\\theta_t$ stationary\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\nabla \\mathcal{L}(\\theta_t)\n",
        "$$\n",
        "\n",
        "-   It is the simplest “first-order” method (i.e., ones using just the\n",
        "    gradient of $\\mathcal{L}$)\n",
        "-   Will call $\\eta_t$ a “learning rate schedule”\n",
        "-   Think of line-search methods as choosing the stepsize $\\eta_t$\n",
        "    optimally. Useful as well for economists, even if used infrequently\n",
        "    in M\n",
        "\n",
        "## When and Where Does This Converge?\n",
        "\n",
        "-   Skipping a million details, see [ProbML Book\n",
        "    1](https://probml.github.io/pml-book/book1.html) Section 8.2.2 and\n",
        "    [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L14.pdf)\n",
        "\n",
        "-   For strictly convex problems this converges to the global minima,\n",
        "    though sufficient conditions include Robbins-Monro\n",
        "    $\\lim_{T\\to \\infty} \\eta_T \\to 0$ and\n",
        "\n",
        "    $$\n",
        "    \\lim_{T\\to\\infty}\\frac{\\sum_{t=1}^T \\eta_t}{\\sum_{t=1}^T \\eta_t^2} = 0\n",
        "    $$\n",
        "\n",
        "-   For problems that not globally convex this may go to local optima,\n",
        "    but if the function is locally strictly convex then it will converge\n",
        "    to a local optima\n",
        "\n",
        "-   For other types of functions (e.g.,\n",
        "    [invex](https://en.wikipedia.org/wiki/Invex_function)) it may still\n",
        "    converge to the “right” solution in some important sense\n",
        "\n",
        "## Preconditioned Gradient Descent\n",
        "\n",
        "-   As we saw analyzing LLS, badly conditioned problems converge slowly\n",
        "    with iterative methods\n",
        "\n",
        "-   We can precondition a problem as we did with linear systems, and it\n",
        "    has the same stationary point\n",
        "\n",
        "-   Choose some $C_t$ for preconditioned gradient descent $$\n",
        "    \\theta_{t+1} = \\theta_t - \\eta_t C_t \\nabla \\mathcal{L}(\\theta_t)\n",
        "    $$\n",
        "\n",
        "-   We saw before that the Hessian tells us the geometry, so the optimal\n",
        "    preconditioner must be related to $\\nabla^2 \\mathcal{L}(\\theta_t)$\n",
        "\n",
        "## Second-Order Methods\n",
        "\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.3\n",
        "-   Adapt $\\eta_t C_t$ to use the Hessian (e.g., Newton’s Method)\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\left[\\nabla^2 \\mathcal{L}(\\theta_t) \\right]^{-1}\\nabla \\mathcal{L}(\\theta_t)\n",
        "$$\n",
        "\n",
        "-   Second order methods are rarer because the calculating the Hessian\n",
        "    is no longer the same computational order as $\\mathcal{L}(\\theta)$\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.3.2 for info on Quasi-Newtonian methods which\n",
        "    approximation Hessian using gradients like BFGS\n",
        "\n",
        "## Momentum\n",
        "\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.2.4\n",
        "-   Can use “momentum”, which speeds up convergence, helps avoid local\n",
        "    optima, and moves fast in flat regions\n",
        "-   Momentum will be a common feature of many ML optimizers (e.g. Adam,\n",
        "    RMSProp, etc.) as it helps with heavily non-convex problems\n",
        "-   A classic method is called Nesterov Accelerated Gradient (NAG),\n",
        "    which is a modification of gradient descent for some\n",
        "    $\\beta_t\\in (0,1)$ (e.g., $0.9$)\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{\\theta}_{t+1} &= \\theta_t + \\beta_t(\\theta_t - \\theta_{t-1})\\\\\n",
        "\\theta_{t+1} &= \\hat{\\theta}_{t+1} - \\eta_t \\nabla \\mathcal{L}(\\hat{\\theta}_{t+1})\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "## Does Uniqueness Matter?\n",
        "\n",
        "-   Remember from our previous lecture on Sobolev norms and\n",
        "    regularization that we care about functions, not parameters.\n",
        "-   Consider when $\\theta$ is used as parameters for a function\n",
        "    (e.g. $\\hat{f}_{\\theta}$)\n",
        "    -   Then what does a lack of convergence of the $\\theta_t$ or\n",
        "        multiplicity with multiple $\\theta$ solutions mean?\n",
        "    -   Maybe nothing! If\n",
        "        $||\\hat{f}_{\\theta_0} - \\hat{f}_{\\theta_1}||_S$ is small, then\n",
        "        the functions themselves may be in the same equivalence class.\n",
        "        Depends on the norm, of course.\n",
        "-   This topic will be discussed when we consider double-descent curves,\n",
        "    but the punchline for now is that the training/optimization is a\n",
        "    means to an end (i.e., generalization) and not an end in itself.\n",
        "\n",
        "## Regularization\n",
        "\n",
        "-   See [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L16.pdf). For\n",
        "    LLS this is the ridge regression\n",
        "-   We discussed regularization as a way to deal with multiplicity\n",
        "\n",
        "$$\n",
        "\\min_{\\theta}\\left[\\mathcal{L}(\\theta) + \\frac{\\alpha}{2} ||\\theta||^2\\right]\n",
        "$$\n",
        "\n",
        "-   Gradient descent becomes (called “weight decay” in ML, and “ridge\n",
        "    regression” if objective is LLS)\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\left[\\nabla \\mathcal{L}(\\theta_t) + \\alpha \\theta_t\\right]\n",
        "$$\n",
        "\n",
        "-   Mapping of regularized $\\theta_t$ to a $f_{\\theta_t}$ is subtle if\n",
        "    nonlinear\n",
        "\n",
        "# Stochastic Optimization\n",
        "\n",
        "## Are Gradients Really that Cheap to Calculate?\n",
        "\n",
        "-   Consider that the objective often involves data (or grid points for\n",
        "    interpolation)\n",
        "    -   Denote $x_n$, and observables $y_n$ for $n=1, \\ldots N$\n",
        "-   With VJPs, the computational order of\n",
        "    $\\nabla_{\\theta} \\mathcal{L}(\\theta;\\{x_n, y_n\\}_{n=1}^N)$ may be\n",
        "    the same as that of $\\mathcal{L}$ itself\n",
        "-   However, keep in mind that reverse-mode requires storing the\n",
        "    intermediate values in the “primal” calculation (i.e.,\n",
        "    $\\mathcal{L}(\\theta;\\{x_n, y_n\\}_{n=1}^N)$)\n",
        "    -   Hence, the memory requirements grow with $N$\n",
        "    -   This may be a big problem for large datasets or complicated\n",
        "        calculations, especially with GPUs which have more limited\n",
        "        memory\n",
        "\n",
        "## Do We Need the Full Gradient?\n",
        "\n",
        "-   In practice, it is impossible to calculate the full gradient for\n",
        "    large datasets\n",
        "\n",
        "-   In GD, the gradient provided the direction of steepest descent\n",
        "\n",
        "-   Consider an algorithm with a $g_t$ as an unbiased estimate of the\n",
        "    gradient\n",
        "\n",
        "    $$\n",
        "    \\begin{aligned}\n",
        "    \\theta_{t+1} = \\theta_t - \\eta_t g_t\\\\\n",
        "    \\mathbb{E}[g_t] = \\nabla \\mathcal{L}(\\theta_t)\n",
        "    \\end{aligned}\n",
        "    $$\n",
        "\n",
        "    -   Make the $\\eta_t$ smaller to deal with noise if this is\n",
        "        high-variance\n",
        "    -   Choose $g_t$ to be far cheaper to calculate than\n",
        "        $\\nabla \\mathcal{L}(\\theta_t)$\n",
        "\n",
        "-   Will turn out that this also adds additional regularization, which\n",
        "    helps with generalization\n",
        "\n",
        "## Stochastic Optimization\n",
        "\n",
        "-   To formalize: Up until now our optimizers have been “deterministic”\n",
        "-   Now we introduce a source of randomness $z \\sim q_{\\theta}(z)$,\n",
        "    i.e. it might depend on the estimated parameters $\\theta$ later with\n",
        "    RL/etc.\n",
        "    -   $z$ could be a source of uncertainty in the environment\n",
        "    -   $z$ could involve latent variables\n",
        "    -   $z$ could come from randomness in the optimization process\n",
        "        (e.g., using subsets of data to form $g_t$)\n",
        "-   Denote expectations using this distribution as\n",
        "    $\\mathbb{E}_{q_{\\theta}(z)}$\n",
        "-   For now, drop the dependence on $\\theta$ for simplicity, though it\n",
        "    becomes crucial for understanding reinforcement learning/etc.\n",
        "\n",
        "## Stochastic Objective\n",
        "\n",
        "-   The full optimization problem is then to minimize this stochastic\n",
        "    objective\n",
        "\n",
        "$$\n",
        "\\min_{\\theta}\\overbrace{\\mathbb{E}_{q(z)} \\tilde{\\mathcal{L}}(\\theta, z)}^{\\equiv \\mathcal{L}(\\theta)}\n",
        "$$\n",
        "\n",
        "-   Under appropriate regularity conditions, could use GD on this\n",
        "    objective\n",
        "\n",
        "$$\n",
        "\\nabla \\mathcal{L}(\\theta) = \\mathbb{E}_{q(z)}\\left[\\nabla \\tilde{\\mathcal{L}}(\\theta, z)\\right]\n",
        "$$\n",
        "\n",
        "-   But in practice, it is rare that we can marginalize out the $z$\n",
        "\n",
        "## Unbiased Draws from the Gradient\n",
        "\n",
        "-   Assume we can sample $z_t \\sim q(z)$ IID\n",
        "-   Then with enough regularity the gradient using just $z_t$ is\n",
        "    unbiased\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{q(z)}\\left[  \\nabla \\tilde{\\mathcal{L}}(\\theta_t, z_t) \\right] = \\nabla \\mathcal{L}(\\theta_t)\n",
        "$$\n",
        "\n",
        "-   That is, on average $\\nabla \\tilde{\\mathcal{L}}(\\theta_t, z_t)$ is\n",
        "    in the right direction for minimizing $\\mathcal{L}(\\theta_t)$\n",
        "-   This basic approach of finding unbiased estimators of the gradient\n",
        "    (and finding ways to lower the variance) is at the heart of most ML\n",
        "    optimization algorithms\n",
        "\n",
        "## Stochastic Gradient Descent\n",
        "\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.4, [ProbML Book\n",
        "    2](https://probml.github.io/pml-book/book2.html) Section 6.3, and\n",
        "    [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L23.pdf)\n",
        "-   Given the previous slide, given IID samples $z_t \\sim q$, the\n",
        "    gradient is unbiased and we have the simplest version of stochastic\n",
        "    gradient descent (SGD)\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\nabla \\tilde{\\mathcal{L}}(\\theta_t, z_t)\n",
        "$$\n",
        "\n",
        "-   Which converges to the minima of $\\min_{\\theta} \\mathcal{L}(\\theta)$\n",
        "    under appropriate conditions\n",
        "-   We can layer on all of the other features we discussed (e.g.,\n",
        "    momentum, preconditioning, etc) with SGD, but some become especially\n",
        "    important (e.g. the $\\eta_t$ schedule)\n",
        "\n",
        "## Finite-Sum Objectives\n",
        "\n",
        "-   Consider a special case of the loss function which is the sum of $N$\n",
        "    terms. For example with empirical risk minimization used in LLS/etc.\n",
        "\n",
        "    -   $z_n \\equiv (x_n, y_n)$ are typically data, observables, or grid\n",
        "        points\n",
        "    -   $\\ell(\\theta, x_n, y_n)$ is a loss function for a single data\n",
        "        point (e.g., forecasting using some $f_{\\theta}$)\n",
        "\n",
        "    $$\n",
        "    \\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{n=1}^N \\tilde{\\mathcal{L}}(\\theta, z_n) \\equiv \\frac{1}{N}\\sum_{n=1}^N \\ell(\\theta, x_n, y_n)\n",
        "    $$\n",
        "\n",
        "    -   For example, LLS is\n",
        "        $\\ell(\\theta, x_n, y_n) = ||y_n - \\theta \\cdot x_n||^2_2$\n",
        "\n",
        "-   In this case, the randomness of $z_t$ is which data point is chosen\n",
        "\n",
        "## SGD for Finite-Sum Objectives\n",
        "\n",
        "-   Hence consider sampling $z_t \\equiv (x_t, y_t)$ from our data.\n",
        "    -   In principle, IID with replacement\n",
        "-   Then run SGD on one data point at a time\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\nabla_{\\theta} \\ell(\\theta_t, x_t, y_t)\n",
        "$$\n",
        "\n",
        "-   This may converges to the minima of $\\mathcal{L}(\\theta)$, and\n",
        "    potentially the storage requirements for calculations the gradient\n",
        "    are radically reduced\n",
        "-   You can guess that the $\\eta_t$ parameter is especially sensitive to\n",
        "    the variance of the gradient estimate\n",
        "\n",
        "## Decrease Variance with Multiple Draws\n",
        "\n",
        "-   With a single draw, the variance of the gradient estimate may be\n",
        "    high\n",
        "\n",
        "$$\n",
        "\\mathbb{E}\\left[\\nabla_{\\theta} \\ell(\\theta_t, x_t, y_t)- \\nabla \\mathcal{L}(\\theta_t)\\right]^2\n",
        "$$\n",
        "\n",
        "-   One tool to decrease the variance is just more monte-carlo draws.\n",
        "    With finite-sum objectives draw $B \\subseteq \\{1,\\ldots N\\}$ indices\n",
        "\n",
        "$$\n",
        "\\frac{1}{|B|}\\sum_{n \\in B} \\nabla_{\\theta} \\ell(\\theta_t, x_n, y_n)\n",
        "$$\n",
        "\n",
        "-   Classic SGD: $|B|=1$; GD: $B = \\{1, \\ldots N\\}$ and in between is\n",
        "    called “minibatch SGD”. Usually minibatch is implied with “SGD”\n",
        "\n",
        "## Minibatch SGD\n",
        "\n",
        "-   Algorithm is to draw $B_t$ indices at each step and execute SGD $$\n",
        "    \\begin{aligned}\n",
        "    g_t \\equiv \\frac{1}{|B_t|}\\sum_{n \\in B_t} \\nabla_{\\theta} \\ell(\\theta_t, x_n, y_n)\\\\\n",
        "    \\theta_{t+1} = \\theta_t - \\eta_t g_t\n",
        "    \\end{aligned}\n",
        "    $$\n",
        "\n",
        "-   Note that we never need to calculate $\\mathcal{L}(\\theta_t)$\n",
        "    directly, so can write our code to all operate on batches $B_t$\n",
        "\n",
        "-   Then layer other tricks on top (e.g., momentum, preconditioning,\n",
        "    etc.)\n",
        "\n",
        "    -   In principle you could also use minibatch with second-order or\n",
        "        quasi-newtonian methods but much rarer\n",
        "\n",
        "## Choosing Batches\n",
        "\n",
        "-   Choosing the $B_t$ process may be tricky. You could sample from\n",
        "    $\\{1,\\ldots N\\}$\n",
        "    -   with replacement\n",
        "    -   without replacement\n",
        "    -   without replacement after shuffling the data, and then ensure\n",
        "        you have gone through all of the data before repeating\n",
        "    -   etc.\n",
        "-   Just remember the goal: variance reduction on gradient estimates\n",
        "-   You want it to be unbiased in principle (consider partitioning the\n",
        "    data into batches and operating sequentially?)\n",
        "-   More art than science in many cases, because it requires many priors\n",
        "\n",
        "## “Grad Student Descent”\n",
        "\n",
        "-   This is how virtually all deep learning works. Just swap SGD with\n",
        "    slightly fancier algorithms using momentum, tinker with parameters,\n",
        "    etc.\n",
        "-   In practice, all of these optimizer settings (e.g., how large for\n",
        "    $|B_t|$, $\\eta_t$, convergence criteria, etc.) are fragile and\n",
        "    require a lot of tuning\n",
        "    -   Part of a a process called **hyperparameter optimization (HPO)**\n",
        "        where you try to find the best non-model parameters for your\n",
        "        goals\n",
        "    -   Same issue with all numerical methods in economics\n",
        "        (e.g. convergence criteria of fixed point iteration, initial\n",
        "        conditions)\n",
        "-   The concern is not just that it is time-consuming for researchers\n",
        "    (and ML “Grad Students”), but that it is easy for priors to sneak in\n",
        "    and bias results\n",
        "\n",
        "## What was our Goal?\n",
        "\n",
        "-   We will address this more formally next lecture, but it is worth\n",
        "    stepping back to think about our goals. Loosely:\n",
        "    -   If we are solving an empirical risk minimization problem (like\n",
        "        regressions, etc.) or interpolation, then our goal is to use the\n",
        "        “data” to find a function $\\hat{f}_{\\theta}$ that is close to\n",
        "        the “true” function $f^*$\n",
        "-   Fitting $\\hat{f}_{\\theta}$ is easy, but we want it to **generalize**\n",
        "    within the true distribution\n",
        "    -   But we don’t know that distribution (hence the “empirical”)\n",
        "    -   So a typical approach is to emulate this by splitting the data\n",
        "        we have\n",
        "    -   But HPO is dangerous because if we are not careful we can\n",
        "        “contaminate” our process for finding $\\hat{f}_{\\theta}$ using\n",
        "        some of the data we intend to check it with. Which might lead to\n",
        "        overfitting/etc.\n",
        "\n",
        "# Training Loops\n",
        "\n",
        "## Splitting the Data\n",
        "\n",
        "A standard way to do this for Empirical Risk\n",
        "Minimization/Regressions/etc. is to split it into three parts:\n",
        "\n",
        "1.  **Training** data used in fitting our approximations\n",
        "    -   This is just a means to an end in ML and economics\n",
        "2.  **Validation** data used for HPO and checking convergence criteria\n",
        "    -   Be cautious to avoid using it for training\n",
        "3.  **Test** data used to evaluate the generalization performance\n",
        "    -   Ensure we don’t accidentally use it in training or validation\n",
        "\n",
        "Not all problems will have this structure, and not all with have\n",
        "validation data.\n",
        "\n",
        "## Why Separate Validation and Test?\n",
        "\n",
        "-   As we will see in deep learning, with massive over-parameterization\n",
        "    you typically can interpolate all of the training data.\n",
        "    -   Minimizing training loss is a means to an end, which usually\n",
        "        ends at zero\n",
        "-   The validation data might be used to check stopping criteria by\n",
        "    checking how well the approximation generalizes to data outside of\n",
        "    training\n",
        "-   But if we are using it for a stopping criteria or HPO, then is is\n",
        "    **contaminated**!\n",
        "    -   Distorts our picture of generalization if we combine it into\n",
        "        test data\n",
        "\n",
        "## What about Interpolation Problems?\n",
        "\n",
        "-   When simply trying to find interpolating functions which solve\n",
        "    functional equations, the risk of prior contamination is less clear\n",
        "-   However, you may still want to separate out validation and test grid\n",
        "    points because any data you use for HPO or convergence criteria\n",
        "    can’t be used to understand generalization.\n",
        "-   For example consider:\n",
        "    1.  Fit until “training” loss is zero\n",
        "    2.  Keep running stochastic optimizer until “validation” loss is\n",
        "        zero\n",
        "-   In that case, it crudely interpolating the validation data, which\n",
        "    makes it equivalent to training data? Not useful for generalization\n",
        "    -   May find that the model generalized better if you **stopped\n",
        "        earlier**\n",
        "\n",
        "## Level of Abstraction for Optimizers\n",
        "\n",
        "-   While you can setup a standard optimization objective and optimizer,\n",
        "    most ML frameworks work at a lower level\n",
        "-   The key reasons are that:\n",
        "    -   Minibatching (usually just called “batches”) requires more\n",
        "        flexibility in implementation to be efficient\n",
        "    -   Stopping criteria is more complicated with highly\n",
        "        overparameterized models\n",
        "    -   Logging and validation logic requires more flexibility\n",
        "    -   Often you will want to take a snapshot of the current best\n",
        "        solution and continue later for refinement (or to solve in\n",
        "        parallel)\n",
        "\n",
        "## Steps and Epochs\n",
        "\n",
        "-   There is a great deal of flexibility in how you setup the optimizer\n",
        "-   But a common approach is to randomly shuffle the data, create a set\n",
        "    of batches $B_t$ (without replacement), and then iterate through\n",
        "    them\n",
        "-   Terminology (when relevant)\n",
        "    -   Every iteration of SGD for a given batch is a **step**\n",
        "    -   If you have gone through the entire dataset once, we say that\n",
        "        you have completed an **epoch**\n",
        "-   At the end of an epoch is a good time to log, check the validation\n",
        "    loss, and potentially stop the training\n",
        "\n",
        "## Software Components used in ML\n",
        "\n",
        "Some common software components for optimization are\n",
        "\n",
        "1.  **Autodifferentiation** and libraries of functions provide the\n",
        "    approximation class\n",
        "2.  **Data loaders** which will take care of providing batches to the\n",
        "    optimizers\n",
        "3.  **Optimizers** are typically iterative, have an internal state, and\n",
        "    you can update with one sample of the gradient for that batch\n",
        "4.  **Logging** and visualization tools to track progress because the\n",
        "    optimization process may be slow and you want to do HPO\n",
        "5.  **HPO** software using training, validation, and possibly test loss\n",
        "\n",
        "## Logging and Visualization\n",
        "\n",
        "-   Several tools exist for logging to babysit optimizers, find good\n",
        "    hyperparameters, etc. including\n",
        "    [Tensorboard](https://www.tensorflow.org/tensorboard)\n",
        "    -   But we will use [Weights and Biases](https://wandb.ai/site)\n",
        "        (W&B) because it is a market leader, free for academics and\n",
        "        seems to be the frontrunner\n",
        "-   Many algorithms and frameworks exist for HPO:\n",
        "    -   [Weights and Biases](https://wandb.ai/site) (W&B) has a built-in\n",
        "        HPO framework using random search and bayesian optimization\n",
        "    -   [Optuna](https://optuna.org/) and [Ray\n",
        "        Tune](https://docs.ray.io/en/master/tune/index.html) is a\n",
        "        popular open-source HPO framework\n",
        "    -   [Ray Tune](https://docs.ray.io/en/master/tune/index.html) is a\n",
        "        popular open-source HPO framework\n",
        "-   HPO frameworks will often use the\n",
        "    [command-line](https://github.com/shadawck/awesome-cli-frameworks#python)\n",
        "    to run new jobs. [Python\n",
        "    Fire](https://github.com/google/python-fire)\n",
        "\n",
        "## Broad Frameworks for Machine Learning\n",
        "\n",
        "-   You can just hand-code loops/etc. which seems the best approach for\n",
        "    JAX\n",
        "    -   Even with Pytorch, it isn’t obvious that a framework is better\n",
        "        ex-post, though ex-ante it can help you try different\n",
        "        permutations easily\n",
        "-   [Pytorch Lightning](https://www.pytorchlightning.ai/) is a popular\n",
        "    framework which will formalize the training loops even across\n",
        "    distributed systems and make CLI, HPO, logging, etc. convenient\n",
        "    -   It remains fairly flexible because it is just wrapping Pytorch\n",
        "-   [Keras](https://keras.io/) is a similar framework with the ability\n",
        "    to target multiple backends (e.g., Pytorch, JAX)\n",
        "    -   The challenge is that it is much less flexible for non-typical\n",
        "        research\n",
        "-   [Hydra](https://github.com/facebookresearch/hydra) is a framework\n",
        "    for more serious engineering code\n",
        "\n",
        "## Linear Regression with SGD in Pytorch\n",
        "\n",
        "See Pytorch implementations of solving LLS in repository\n",
        "\n",
        "1.  Baseline: GD with full gradient\n",
        "2.  SGD with minibatches\n",
        "3.  Linear Model as a “Neural Network”\n",
        "4.  Logging\n",
        "5.  CLI\n",
        "\n",
        "## Linear Regression with SGD in JAX\n",
        "\n",
        "1.  Using JAX and `vmap`\n",
        "2.  Using JAX and equinox"
      ],
      "id": "23c14b15-24b6-4abd-bb08-892faa19fa55"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}