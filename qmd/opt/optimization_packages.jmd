---
title       : "Optimization Packages"
subtitle    : "ECON622"
author      : Paul Schrimpf
date        : `j using Dates; print(Dates.today())`
weave_options:
      out_width : 100%
      wrap : true
      fig_width : 800
      fig_height : 800
      dpi : 192
---

# Optimization


## Example Problems

```julia
using Statistics, LinearAlgebra

function cueiv(β; n=1000, σ=0.1, γ=[I; ones(2,length(β))], ρ=0.5)
  z = randn(n, size(γ)[1])
  endo = randn(n, length(β))
  x = z*γ .+ endo
  ϵ = σ*(randn(n)*sqrt(1.0-ρ^2).+endo[:,1]*ρ)
  y =  x*β .+ ϵ
  g(β) = (y - x*β).*z
  function cueobj(β)
    G = g(β)
    Eg=mean(G,dims=1)
    W = inv(cov(G))
    (n*Eg*W*Eg')[1]
  end
end

function nonlinearLS(β; n=1000, σ=0.1, f = (x,β)->sin(x'*β)/(1+exp(x'*β)))
  x=rand(n,length(β))
  ϵ = randn(n)*σ
  y = mapslices(x->f(x,β),x,dims=2) + ϵ
  nnls(β)=mean(e->e^2, y-mapslices(x->f(x,β),x,dims=2))
end
```

# Packages

We focus on algorithms useful for small to medium scale smooth
nonlinear problems. Variants of Newton's method will generally be the
best choice for such problems.

## Optim.jl

Pure Julia implementation of standard set of optimization algorithms. This is generally a good starting point, especially
for unconstrained nonlinear problems without too many variables (maybe up to a few hundred).

- [Optim docs](https://julianlsolvers.github.io/Optim.jl/stable/)

```julia
using Plots
import Optim
β = ones(2)
f = cueiv(β;σ=0.1,ρ=0.5)
b1 = range(0,2,length=100)
b2 = range(0,2,length=100)
fval = ((x,y)->f([x,y])).(b1,b2')
Plots.plotly()
contourf(b1,b2,fval)
β0 = zero(β)

sol=Optim.optimize(f, β0, Optim.NelderMead())
sol=Optim.optimize(f,β0, Optim.LBFGS(), autodiff=:forward)
sol=Optim.optimize(f,β0, Optim.LBFGS(m=20, linesearch=Optim.LineSearches.BackTracking()), autodiff=:forward)
sol=Optim.optimize(f,β0,Optim.NewtonTrustRegion(), autodiff=:forward)

β = ones(20)
f =  cueiv(β)
β0 = zero(β)
sol=Optim.optimize(f, β0, Optim.NelderMead())
sol=Optim.optimize(f,β0, Optim.LBFGS(), autodiff=:forward)
sol=Optim.optimize(f,β0, Optim.LBFGS(m=20, linesearch=Optim.LineSearches.BackTracking()), autodiff=:forward)
sol=Optim.optimize(f,β0,Optim.NewtonTrustRegion(), autodiff=:forward)
```

## Optimization.jl

A package that provides a uniform interface to many other optimization packages. A good choice for easy experimentation and trying different packages.

- [Optimization docs](https://docs.sciml.ai/Optimization/stable/)

## JuliaSmoothOptimizers

A smaller set of developers and users than Optim, but has some good
ideas and algorithms. Multiple options for nonlinear constrained
optimization, [some with promising
benchmarks.](https://jso.dev/DCISolver.jl/stable/benchmark/#CUTEst-benchmark-with-Knitro). Design
makes using specialized linear solvers inside nonlinear algorithms
convenient.

- [Organization README](https://github.com/JuliaSmoothOptimizers)

```julia
using ADNLPModels
import DCISolver, JSOSolvers, Percival
β = ones(2)
f =  cueiv(β; γ = I + zeros(2,2))
β0 = zero(β)

nlp = ADNLPModel(f, β0)
stats=JSOSolvers.lbfgs(nlp)
print(stats)

stats=JSOSolvers.tron(nlp)
print(stats)

ecnlp = ADNLPModel(f,β0, b->(length(β0) - sum(b)), zeros(1), zeros(1))
stats=DCISolver.dci(ecnlp)
print(stats)

icnlp = ADNLPModel(f,β0, b->(length(β0) - sum(b)), -ones(1), ones(1))
stats=Percival.percival(icnlp)
print(stats)
```

## Ipopt

An open source solver written in C (I think) that works well on large constrained nonlinear problems. Can be used directly or through JuMP, Optimization.jl or JuliaSmoothOptimizers interfaces.

## Knitro

A commercial solver with state of the art performance on many problems. Maybe worth the cost for large constrained nonlinear problems. Can be used directly or through JuMP, Optimization.jl or JuliaSmoothOptimizers interfaces.

## JuMP

[JuMP](https://jump.dev/) is a modelling language for optimization in Julia. It has can be used in conjunction with many solvers. To use it, you must write your problem in the special syntax of JuMP. JuMP is then able to efficiently calculate derivatives and recognize special problem structure such as linearity, sparsity, etc.

## Optimisers

[Optimisers.jl](https://fluxml.ai/Optimisers.jl/dev/) has a collection of gradient descent variations designed for use in machine learning. A reasonable choice for problems with a very large number of variables.

## Covariance Matrix Adaption Evolution Strategy

CMAES can be effective for problems with many local minimum while retaining some of the benefits of Newton's method. It can even deal with noisy and discontinuous objective functions. [CMAEvolutionStrategy](https://github.com/jbrea/CMAEvolutionStrategy.jl) is a Julia implementation. [GCMAES](https://github.com/AStupidBear/GCMAES.jl) implements both the original CMAES algorithm and a variation that also uses gradient information.

```julia
import CMAEvolutionStrategy, GCMAES
β = ones(10)
f =  cueiv(β; γ = I + zeros(length(β),length(β)))
β0 = zero(β)

out = CMAEvolutionStrategy.minimize(f, β0, 1.0;
                                    lower = nothing,
                                    upper = nothing,
                                    popsize = 4 + floor(Int, 3*log(length(β0))),
                                    verbosity = 1,
                                    seed = rand(UInt),
                                    maxtime = nothing,
                                    maxiter = 500,
                                    maxfevals = nothing,
                                    ftarget = nothing,
                                    xtol = nothing,
                                    ftol = 1e-11)
@show CMAEvolutionStrategy.xbest(out)

xmin, fmin, status = GCMAES.minimize(f, β0, 1.0, fill(-10.,length(β0)), fill(10., length(β0)), maxiter=500)
@show xmin

import ForwardDiff
∇f(β) = ForwardDiff.gradient(f, β)
xmin, fmin, status=GCMAES.minimize((f, ∇f), β0, 1.0, fill(-10.,length(β0)), fill(10., length(β0)), maxiter=500)
@show xmin
```
