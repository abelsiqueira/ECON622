trainer:
  accelerator: cpu
  max_epochs: 500
  min_epochs: 0
  max_time: 00:00:10:00
  precision: 32
  num_sanity_val_steps: 0
  logger:
    class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      offline: false # set to true to not upload during testing
      log_model: false # set to true to save the model at the end
      name: null # can set name or have it automatically generated
      project: linear_regression_pytorch
      group: null # can group related runs
  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
        log_momentum: false
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        filename: best
        monitor: train_loss
        verbose: false
        save_last: true
        save_top_k: 1
        save_weights_only: true
        mode: min
        auto_insert_metric_name: true
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: val_loss
        min_delta: 0.0
        patience: 50
        mode: min
        check_finite: true
        divergence_threshold: 100000 # stops if larger
        stopping_threshold: 1.0e-8   # typically the binding stopping threshold
optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 1.0e-3
lr_scheduler:
  class_path: torch.optim.lr_scheduler.StepLR
  init_args:
    step_size: 100
    gamma: 0.9   
model:
  N: 1000
  M: 2
  sigma: 0.001
  train_prop: 0.7
  val_prop: 0.15
  batch_size: 32